Authors,ArticleTitle,Source Title,Volume,Issue,Start Page,End Page,DOI,Early Access Date,Document Type,Publication Date,Publication Year,Abstract,N,O
"Cao, Xubo; Kosinski, Michal",Large language models and humans converge in judging public figures' personalities,PNAS NEXUS,3,10,,,10.1093/pnasnexus/pgae418,,Article,OCT 1 2024,2024,"ChatGPT-4 and 600 human raters evaluated 226 public figures' personalities using the Ten-Item Personality Inventory. The correlation between ChatGPT-4 and aggregate human ratings ranged from r = 0.76 to 0.87, outperforming the models specifically trained to make such predictions. Notably, the model was not provided with any training data or feedback on its performance. We discuss the potential explanations and practical implications of ChatGPT-4's ability to mimic human responses accurately.",AI Performance vs. Human Benchmarks,ChatGPT-4在用十项人格量表评估226名公众人物人格上与600名人类评分者高度一致（r=0.76–0.87），胜过专门训练的模型。
"Tao, Yan; Viberg, Olga; Baker, Ryan S.; Kizilcec, Rene F.",Cultural bias and cultural alignment of large language models,PNAS NEXUS,3,9,,,10.1093/pnasnexus/pgae346,,Article,SEP 17 2024,2024,"Culture fundamentally shapes people's reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people's authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models' responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models' output for 71-81% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI.","Bias, Ethics, and Societal Risks",五种主流LLM展现英美/新教欧洲文化倾向，文化提示可在71–81%国家改善模型的文化对齐。
"Heersmink, Richard",Use of large language models might affect our cognitive skills,NATURE HUMAN BEHAVIOUR,8,5,805,806,10.1038/s41562-024-01859-y,24-Mar,Editorial Material,24-May,2024,"Large language models can generate sophisticated text or code with little input from a user, which has the potential to impoverish our own writing and thinking skills. We need to understand the effect of this technology on our cognition and to decide whether this is what we want.",Impact on Human Cognition and Collective Intelligence,LLM能生成高级文本与代码，可能削弱人类写作与思维技能，需要评估其对认知的长期影响。
"Heide-Jorgensen, Tobias; Eady, Gregory; Rasmussen, Anne",Understanding the success and failure of online political debate: Experimental evidence using large language models,SCIENCE ADVANCES,11,30,,,10.1126/sciadv.adv7864,,Article,JUL 23 2025,2025,"Online political debate is frequently lamented for being toxic, partisan, and counterproductive. However, we know little about how core elements of political debate (justification, tone, willingness to compromise, and partisanship) affect its quality. Using text-based treatments experimentally manipulated with a large language model, we test how these elements causally affect the quality of open-text responses about issues important to the US and UK public. We find substantial evidence that differences in justification, tone, and willingness to compromise, but not partisanship, affect the quality of subsequent discourse. Combined, these elements increase the probability of high-quality responses by roughly 1.6 to 2 times and substantially increase openness to alternative viewpoints. Despite the ability to bring about substantial changes in discourse quality, we find no evidence of changes in political attitudes themselves. Our findings demonstrate how adapting approaches to online debate can foster healthy democratic interactions but have less influence on changing minds.",Impact on Human Cognition and Collective Intelligence,用LLM操控论证、语气与妥协意愿能显著提升公开政治讨论质量与观点开放性，但不改变政治立场。
"Abdurahman, Suhaib; Atari, Mohammad; Karimi-Malekabadi, Farzan; Xue, Mona J.; Trager, Jackson; Park, Peter S.; Golazizian, Preni; Omrani, Ali; Dehghani, Morteza",Perils and opportunities in using large language models in psychological research,PNAS NEXUS,3,7,,,10.1093/pnasnexus/pgae245,24-Jul,Article,JUL 16 2024,2024,"The emergence of large language models (LLMs) has sparked considerable interest in their potential application in psychological research, mainly as a model of the human psyche or as a general text-analysis tool. However, the trend of using LLMs without sufficient attention to their limitations and risks, which we rhetorically refer to as GPTology, can be detrimental given the easy access to models such as ChatGPT. Beyond existing general guidelines, we investigate the current limitations, ethical implications, and potential of LLMs specifically for psychological research, and show their concrete impact in various empirical studies. Our results highlight the importance of recognizing global psychological diversity, cautioning against treating LLMs (especially in zero-shot settings) as universal solutions for text analysis, and developing transparent, open methods to address LLMs' opaque nature for reliable, reproducible, and robust inference from AI-generated data. Acknowledging LLMs' utility for task automation, such as text annotation, or to expand our understanding of human psychology, we argue for diversifying human samples and expanding psychology's methodological toolbox to promote an inclusive, generalizable science, countering homogenization, and over-reliance on LLMs.","Bias, Ethics, and Societal Risks",讨论LLM在心理学研究中的潜力与风险，强调避免零样本过度依赖、重视文化多样性与方法透明性。
"Tyler, Chris; Akerlof, K. L.; Allegra, Alessandro; Arnold, Zachary; Canino, Henriette; Doornenbal, Marius A.; Goldstein, Josh A.; Pedersen, David Budtz; Sutherland, William J.",AI tools as science policy advisers? The potential and the pitfalls,NATURE,,,,,10.1038/d41586-023-02999-3,23-Sep,Editorial Material; Early Access,,2023,Large language models and other artificial-intelligence systems could be excellent at synthesizing scientific evidence for policymakers - but only with appropriate safeguards and humans in the loop. Large language models and other artificial-intelligence systems could be excellent at synthesizing scientific evidence for policymakers - but only with appropriate safeguards and humans in the loop.,Applications in Professional and Scientific Fields,LLM可为政策制定者综合科学证据提供帮助，但必须有人类参与与适当保障措施。
"Burton, Jason W.; Lopez-Lopez, Ezequiel; Hechtlinger, Shahar; Rahwan, Zoe; Aeschbach, Samuel; Bakker, Michiel A.; Becker, Joshua A.; Berditchevskaia, Aleks; Berger, Julian; Brinkmann, Levin; Flek, Lucie; Herzog, Stefan M.; Huang, Saffron; Kapoor, Sayash; Narayanan, Arvind; Nussberger, Anne-Marie; Yasseri, Taha; Nickl, Pietro; Almaatouq, Abdullah; Hahn, Ulrike; Kurvers, Ralf H. J. M.; Leavy, Susan; Rahwan, Iyad; Siddarth, Divya; Siu, Alice; Woolley, Anita W.; Wulff, Dirk U.; Hertwig, Ralph",How large language models can reshape collective intelligence,NATURE HUMAN BEHAVIOUR,8,9,1643,1655,10.1038/s41562-024-01959-9,24-Sep,Review,24-Sep,2024,"Collective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals-even experts-resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the 'wisdom of crowds', online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans' ability to collectively tackle complex problems.",Impact on Human Cognition and Collective Intelligence,探讨LLM如何重塑信息聚合與传播，既可能增强集体智慧也带来风险与政策挑战。
"Wang, Yifei; Eshghi, Ashkan; Ding, Yi; Gopal, Ram",Echoes of authenticity: Reclaiming human sentiment in the large language model era,PNAS NEXUS,4,2,,,10.1093/pnasnexus/pgaf034,,Article,FEB 25 2025,2025,"This paper scrutinizes the unintended consequences of employing large language models (LLMs) like ChatGPT for editing user-generated content (UGC), particularly focusing on alterations in sentiment. Through a detailed analysis of a climate change tweet dataset, we uncover that LLM-rephrased tweets tend to display a more neutral sentiment than their original counterparts. By replicating an established study on public opinions regarding climate change, we illustrate how such sentiment alterations can potentially skew the results of research relying on UGC. To counteract the biases introduced by LLMs, our research outlines two effective strategies. First, we employ predictive models capable of retroactively identifying the true human sentiment underlying the original communications, utilizing the altered sentiment expressed in LLM-rephrased tweets as a basis. While useful, this approach faces limitations when the origin of the text-whether directly crafted by a human or modified by an LLM-remains uncertain. To address such scenarios where the text's provenance is ambiguous, we develop a second approach based on the fine-tuning of LLMs. This fine-tuning process not only helps in aligning the sentiment of LLM-generated texts more closely with human sentiment but also offers a robust solution to the challenges posed by the indeterminate origins of digital content. This research highlights the impact of LLMs on the linguistic characteristics and sentiment of UGC, and more importantly, offers practical solutions to mitigate these biases, thereby ensuring the continued reliability of sentiment analysis in research and policy.",Content Authenticity and Mitigation Strategies,LLM重写的用户生成内容趋于中性化，可能扭曲情感研究，作者提出基于预测模型与微调的两种缓解方法。
"Wack, Morgan; Ehrett, Carl; Linvill, Darren; Warren, Patrick",Generative propaganda: Evidence of AI's impact from a state-backed disinformation campaign,PNAS NEXUS,4,4,,,10.1093/pnasnexus/pgaf083,,Article,25-Apr,2025,"Can AI bolster state-backed propaganda campaigns, in practice? Growing use of AI and large language models has drawn attention to the potential for accompanying tools to be used by malevolent actors. Though recent laboratory and experimental evidence has substantiated these concerns in principle, the usefulness of AI tools in the production of propaganda campaigns has remained difficult to ascertain. Drawing on the adoption of generative-AI techniques by a state-affiliated propaganda site with ties to Russia, we test whether AI adoption enabled the website to amplify and enhance its production of disinformation. First, we find that the use of generative-AI tools facilitated the outlet's generation of larger quantities of disinformation. Second, we find that use of generative-AI coincided with shifts in the volume and breadth of published content. Finally, drawing on a survey experiment comparing perceptions of articles produced prior to and following the adoption of AI tools, we show that the AI-assisted articles maintained their persuasiveness in the postadoption period. Our results illustrate how generative-AI tools have already begun to alter the size and scope of state-backed propaganda campaigns.","Bias, Ethics, and Societal Risks",证据显示与俄罗斯相关的宣传网站采用生成式AI后扩大了虚假信息产出规模与广度，并维持其说服力。
"Rubin, Matan; Li, Joanna Z.; Zimmerman, Federico; Ong, Desmond C.; Goldenberg, Amit; Perry, Anat",Comparing the value of perceived human versus AI-generated empathy,NATURE HUMAN BEHAVIOUR,,,,,10.1038/s41562-025-02247-w,25-Jun,Article; Early Access,,2025,"Artificial intelligence (AI) and specifically large language models demonstrate remarkable social-emotional abilities, which may improve human-AI interactions and AI's emotional support capabilities. However, it remains unclear whether empathy, encompassing understanding, 'feeling with' and caring, is perceived differently when attributed to AI versus humans. We conducted nine studies (n = 6,282) where AI-generated empathic responses to participants' emotional situations were labelled as provided by either humans or AI. Human-attributed responses were rated as more empathic and supportive, and elicited more positive and fewer negative emotions, than AI-attributed ones. Moreover, participants' own uninstructed belief that AI had aided the human-attributed responses reduced perceived empathy and support. These effects were replicated across varying response lengths, delays, iterations and large language models and were primarily driven by responses emphasizing emotional sharing and care. Additionally, people consistently chose human interaction over AI when seeking emotional engagement. These findings advance our general understanding of empathy, and specifically human-AI empathic interactions.",Human-AI Interaction and Perception,当AI生成的同理回应被标为人类时被评为更有同理心且更支持，参与者普遍偏好与人类互动以寻求情感支持。
"Prillaman, Mckenzie",Is ChatGPT making scientists hyper-productive? The highs and lows of using AI,NATURE,627,8002,16,17,10.1038/d41586-024-00592-w,,News Item,MAR 7 2024,2024,Large language models are transforming scientific writing and publishing. But the productivity boost that these tools bring could have a downside.,Applications in Professional and Scientific Fields,LLM提升科学写作与发表的生产力，但这种增产可能伴随潜在负面后果。
"McDuff, Daniel; Schaekermann, Mike; Tu, Tao; Palepu, Anil; Wang, Amy; Garrison, Jake; Singhal, Karan; Sharma, Yash; Azizi, Shekoofeh; Kulkarni, Kavita; Hou, Le; Cheng, Yong; Liu, Yun; Mahdavi, S. Sara; Prakash, Sushant; Pathak, Anupam; Semturs, Christopher; Patel, Shwetak; Webster, Dale R.; Dominowska, Ewa; Gottweis, Juraj; Barral, Joelle; Chou, Katherine; Corrado, Greg S.; Matias, Yossi; Sunshine, Jake; Karthikesalingam, Alan; Natarajan, Vivek",Towards accurate differential diagnosis with large language models,NATURE,642,8067,,,10.1038/s41586-025-08869-4,25-Apr,Article,JUN 12 2025,2025,"A comprehensive differential diagnosis is a cornerstone of medical care that is often reached through an iterative process of interpretation that combines clinical history, physical examination, investigations and procedures. Interactive interfaces powered by large language models present new opportunities to assist and automate aspects of this process1. Here we introduce the Articulate Medical Intelligence Explorer (AMIE), a large language model that is optimized for diagnostic reasoning, and evaluate its ability to generate a differential diagnosis alone or as an aid to clinicians. Twenty clinicians evaluated 302 challenging, real-world medical cases sourced from published case reports. Each case report was read by two clinicians, who were randomized to one of two assistive conditions: assistance from search engines and standard medical resources; or assistance from AMIE in addition to these tools. All clinicians provided a baseline, unassisted differential diagnosis prior to using the respective assistive tools. AMIE exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1% versus 33.6%, P = 0.04). Comparing the two assisted study arms, the differential diagnosis quality score was higher for clinicians assisted by AMIE (top-10 accuracy 51.7%) compared with clinicians without its assistance (36.1%; McNemar's test: 45.7, P < 0.01) and clinicians with search (44.4%; McNemar's test: 4.75, P = 0.03). Further, clinicians assisted by AMIE arrived at more comprehensive differential lists than those without assistance from AMIE. Our study suggests that AMIE has potential to improve clinicians' diagnostic reasoning and accuracy in challenging cases, meriting further real-world evaluation for its ability to empower physicians and widen patients' access to specialist-level expertise.",Applications in Professional and Scientific Fields,优化诊断推理的AMIE在302个真实复杂病例中的top-10准确率优于未辅助手的临床医生，能改善差异诊断质量与全面性。
"Goldstein, Josh A.; Chao, Jason; Grossman, Shelby; Stamos, Alex; Tomz, Michael",How persuasive is AI-generated propaganda?,PNAS NEXUS,3,2,,,10.1093/pnasnexus/pgae034,24-Feb,Article,FEB 1 2024,2024,"Can large language models, a form of artificial intelligence (AI), generate persuasive propaganda? We conducted a preregistered survey experiment of US respondents to investigate the persuasiveness of news articles written by foreign propagandists compared to content generated by GPT-3 davinci (a large language model). We found that GPT-3 can create highly persuasive text as measured by participants' agreement with propaganda theses. We further investigated whether a person fluent in English could improve propaganda persuasiveness. Editing the prompt fed to GPT-3 and/or curating GPT-3's output made GPT-3 even more persuasive, and, under certain conditions, as persuasive as the original propaganda. Our findings suggest that propagandists could use AI to create convincing content with limited effort.","Bias, Ethics, and Societal Risks",GPT-3能生成高度说服力的宣传文本，经过提示调整或人工编辑后在某些情形下可与原始宣传同样具说服力。
"Strachan, James W. A.; Albergo, Dalila; Borghini, Giulia; Pansardi, Oriana; Scaliti, Eugenio; Gupta, Saurabh; Saxena, Krati; Rufo, Alessandro; Panzeri, Stefano; Manzi, Guido; Graziano, Michael S. A.; Becchio, Cristina",Testing theory of mind in large language models and humans,NATURE HUMAN BEHAVIOUR,8,7,,,10.1038/s41562-024-01882-z,24-May,Article,24-Jul,2024,"At the core of what defines us as humans is the concept of theory of mind: the ability to track other people's mental states. The recent development of large language models (LLMs) such as ChatGPT has led to intense debate about the possibility that these models exhibit behaviour that is indistinguishable from human behaviour in theory of mind tasks. Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities, from understanding false beliefs to interpreting indirect requests and recognizing irony and faux pas. We tested two families of LLMs (GPT and LLaMA2) repeatedly against these measures and compared their performance with those from a sample of 1,907 human participants. Across the battery of theory of mind tests, we found that GPT-4 models performed at, or even sometimes above, human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas. Faux pas, however, was the only test where LLaMA2 outperformed humans. Follow-up manipulations of the belief likelihood revealed that the superiority of LLaMA2 was illusory, possibly reflecting a bias towards attributing ignorance. By contrast, the poor performance of GPT originated from a hyperconservative approach towards committing to conclusions rather than from a genuine failure of inference. These findings not only demonstrate that LLMs exhibit behaviour that is consistent with the outputs of mentalistic inference in humans but also highlight the importance of systematic testing to ensure a non-superficial comparison between human and artificial intelligences.Testing two families of large language models (LLMs) (GPT and LLaMA2) on a battery of measurements spanning different theory of mind abilities, Strachan et al. find that the performance of LLMs can mirror that of humans on most of these tasks. The authors explored potential reasons for this.",AI Performance vs. Human Benchmarks,在多项心智理论任务中，GPT系列与LLaMA2在多数测验上能匹配或超越人类表现，但在某些子测验存在偏差与策略差异。
"Pataranutaporn, Pat; Liu, Ruby; Finn, Ed; Maes, Pattie","Influencing human-AI interaction by priming beliefs about AI can increase perceived trustworthiness, empathy and effectiveness",NATURE MACHINE INTELLIGENCE,5,10,1076,1086,10.1038/s42256-023-00720-7,23-Oct,Article,23-Oct,2023,"As conversational agents powered by large language models become more human-like, users are starting to view them as companions rather than mere assistants. Our study explores how changes to a person's mental model of an AI system affects their interaction with the system. Participants interacted with the same conversational AI, but were influenced by different priming statements regarding the AI's inner motives: caring, manipulative or no motives. Here we show that those who perceived a caring motive for the AI also perceived it as more trustworthy, empathetic and better-performing, and that the effects of priming and initial mental models were stronger for a more sophisticated AI model. Our work also indicates a feedback loop in which the user and AI reinforce the user's mental model over a short time; further work should investigate long-term effects. The research highlights the importance of how AI systems are introduced can notably affect the interaction and how the AI is experienced.The recent accessibility of large language models brought them into contact with a large number of users and, due to the social nature of language, it is hard to avoid prescribing human characteristics such as intentions to a chatbot. Pataranutaporn and colleagues investigated how framing a bot as helpful or manipulative can influence this perception and the behaviour of the humans that interact with it.",Human-AI Interaction and Perception,将AI描绘为有“关怀”动机可增加用户对其信任、同理心与效能感，且这种效果在更先进模型上更明显。
"Liang, Weixin; Zhang, Yaohui; Wu, Zhengxuan; Lepp, Haley; Ji, Wenlong; Zhao, Xuandong; Cao, Hancheng; Liu, Sheng; He, Siyu; Huang, Zhi; Yang, Diyi; Potts, Christopher; Manning, Christopher D.; Zou, James",Quantifying large language model usage in scientific papers,NATURE HUMAN BEHAVIOUR,,,,,10.1038/s41562-025-02273-8,25-Aug,Article; Early Access,,2025,"Scientific publishing is the primary means of disseminating research findings. There has been speculation about how extensively large language models (LLMs) are being used in academic writing. Here we conduct a systematic analysis across 1,121,912 preprints and published papers from January 2020 to September 2024 on arXiv, bioRxiv and Nature portfolio journals, using a population-level framework based on word frequency shifts to estimate the prevalence of LLM-modified content over time. Our findings suggest a steady increase in LLM usage, with the largest and fastest growth estimated for computer science papers (up to 22%). By comparison, mathematics papers and the Nature portfolio showed lower evidence of LLM modification (up to 9%). LLM modification estimates were higher among papers from first authors who post preprints more frequently, papers in more crowded research areas and papers of shorter lengths. Our findings suggest that LLMs are being broadly used in scientific writing.",Applications in Professional and Scientific Fields,基于对2020–2024年逾112万篇论文的词频分析表明，LLM在学术写作中的使用稳步上升，计算机科学增长最快（最高约22%）。
"Zhou, Lexin; Schellaert, Wout; Martinez-Plumed, Fernando; Moros-Daval, Yael; Ferri, Cesar; Hernandez-Orallo, Jose",Larger and more instructable language models become less reliable,NATURE,634,8032,61,+,10.1038/s41586-024-07930-y,24-Sep,Article,OCT 3 2024,2024,"The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources(1)) and bespoke shaping up (including post-filtering(2,3), fine tuning or use of human feedback(4,5)). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount.","Bias, Ethics, and Societal Risks",研究发现规模更大、越可指令化的模型在可预测错误分布和被人工监督识别错误方面反而更不可靠，需重新设计以适应高风险场景。
"Akata, Elif; Schulz, Lion; Coda-Forno, Julian; Oh, Seong Joon; Bethge, Matthias; Schulz, Eric",Playing repeated games with large language models,NATURE HUMAN BEHAVIOUR,9,7,,,10.1038/s41562-025-02172-y,25-May,Article,25-Jul,2025,"Large language models (LLMs) are increasingly used in applications where they interact with humans and other agents. We propose to use behavioural game theory to study LLMs' cooperation and coordination behaviour. Here we let different LLMs play finitely repeated 2 x 2 games with each other, with human-like strategies, and actual human players. Our results show that LLMs perform particularly well at self-interested games such as the iterated Prisoner's Dilemma family. However, they behave suboptimally in games that require coordination, such as the Battle of the Sexes. We verify that these behavioural signatures are stable across robustness checks. We also show how GPT-4's behaviour can be modulated by providing additional information about its opponent and by using a 'social chain-of-thought' strategy. This also leads to better scores and more successful coordination when interacting with human players. These results enrich our understanding of LLMs' social behaviour and pave the way for a behavioural game theory for machines.",Human-AI Interaction and Perception,LLM在自利型重复博弈（如囚徒困境）表现良好但在需要协调的博弈中较差，提供对手信息与‘社会思路链’可改善其协调能力。
"Markowitz, David M.",From complexity to clarity: How AI enhances perceptions of scientists and the public's understanding of science,PNAS NEXUS,3,9,,,10.1093/pnasnexus/pgae387,,Article,SEP 16 2024,2024,"This article evaluated the effectiveness of using generative AI to simplify science communication and enhance the public's understanding of science. By comparing lay summaries of journal articles from PNAS, yoked to those generated by AI, this work first assessed linguistic simplicity differences across such summaries and public perceptions in follow-up experiments. Specifically, study 1a analyzed simplicity features of PNAS abstracts (scientific summaries) and significance statements (lay summaries), observing that lay summaries were indeed linguistically simpler, but effect size differences were small. Study 1b used a large language model, GPT-4, to create significance statements based on paper abstracts and this more than doubled the average effect size without fine-tuning. Study 2 experimentally demonstrated that simply-written generative pre-trained transformer (GPT) summaries facilitated more favorable perceptions of scientists (they were perceived as more credible and trustworthy, but less intelligent) than more complexly written human PNAS summaries. Crucially, study 3 experimentally demonstrated that participants comprehended scientific writing better after reading simple GPT summaries compared to complex PNAS summaries. In their own words, participants also summarized scientific papers in a more detailed and concrete manner after reading GPT summaries compared to PNAS summaries of the same article. AI has the potential to engage scientific communities and the public via a simple language heuristic, advocating for its integration into scientific dissemination for a more informed society.",Applications in Professional and Scientific Fields,GPT-4生成的简明科学摘要提高公众理解并使科学家被视为更可信但显得不那么聪明，促进更详细的受众再表述。
"Liu, Wei; Xiang, Ming; Ding, Nai",Active use of latent tree-structured sentence representation in humans and large language models,NATURE HUMAN BEHAVIOUR,,,,,10.1038/s41562-025-02297-0,25-Sep,Article; Early Access,,2025,"Understanding how sentences are represented in the human brain, as well as in large language models (LLMs), poses a substantial challenge for cognitive science. Here we develop a one-shot learning task to investigate whether humans and LLMs encode tree-structured constituents within sentences. Participants (total N = 372, native Chinese or English speakers, and bilingual in Chinese and English) and LLMs (for example, ChatGPT) were asked to infer which words should be deleted from a sentence. Both groups tend to delete constituents, instead of non-constituent word strings, following rules specific to Chinese and English, respectively. The results cannot be explained by models that rely only on word properties and word positions. Crucially, based on word strings deleted by either humans or LLMs, the underlying constituency tree structure can be successfully reconstructed. Altogether, these results demonstrate that latent tree-structured sentence representations emerge in both humans and LLMs.",Impact on Human Cognition and Collective Intelligence,人类与LLM在一句话删除任务中均倾向删除句子构成成分，表明树状句法表征在两者中皆会出现。
"Cai, Shanqing; Venugopalan, Subhashini; Seaver, Katie; Xiao, Xiang; Tomanek, Katrin; Jalasutram, Sri; Morris, Meredith Ringel; Kane, Shaun; Narayanan, Ajit; MacDonald, Robert L.; Kornman, Emily; Vance, Daniel; Casey, Blair; Gleason, Steve M.; Nelson, Philip Q.; Brenner, Michael P.",Using large language models to accelerate communication for eye gaze typing users with ALS,NATURE COMMUNICATIONS,15,1,,,10.1038/s41467-024-53873-3,,Article,NOV 1 2024,2024,"Accelerating text input in augmentative and alternative communication (AAC) is a long-standing area of research with bearings on the quality of life in individuals with profound motor impairments. Recent advances in large language models (LLMs) pose opportunities for re-thinking strategies for enhanced text entry in AAC. In this paper, we present SpeakFaster, consisting of an LLM-powered user interface for text entry in a highly-abbreviated form, saving 57% more motor actions than traditional predictive keyboards in offline simulation. A pilot study on a mobile device with 19 non-AAC participants demonstrated motor savings in line with simulation and relatively small changes in typing speed. Lab and field testing on two eye-gaze AAC users with amyotrophic lateral sclerosis demonstrated text-entry rates 29-60% above baselines, due to significant saving of expensive keystrokes based on LLM predictions. These findings form a foundation for further exploration of LLM-assisted text entry in AAC and other user interfaces.Individuals with severe motor impairments use gaze to type and communicate. This paper presents a large language model-based user interface that enables gaze typing in highly abbreviated forms, achieving significant motor saving and speed gain.",Applications in Professional and Scientific Fields,LLM驱动的SpeakFaster界面显著减少眼动打字所需动作并提高ALS用户输入速度。
"Ghassemi, Marzyeh",Presentation matters for AI-generated clinical advice,NATURE HUMAN BEHAVIOUR,,,,,10.1038/s41562-023-01721-7,23-Nov,Editorial Material; Early Access,,2023,"If mistakes are made in clinical settings, patients suffer. Artificial intelligence (AI) generally - and large language models specifically - are increasingly used in health settings, but the way that physicians use AI tools in this high-stakes environment depends on how information is delivered. AI toolmakers have a responsibility to present information in a way that minimizes harm.",Human-AI Interaction and Perception,临床环境中AI建议的呈现方式影响医师使用与患者安全，工具设计者需以减害为责。
"Kirk, Hannah Rose; Vidgen, Bertie; Rottger, Paul; Hale, Scott A.","The benefits, risks and bounds of personalizing the alignment of large language models to individuals",NATURE MACHINE INTELLIGENCE,6,4,383,392,10.1038/s42256-024-00820-y,,Review,24-Apr,2024,"Large language models (LLMs) undergo 'alignment' so that they better reflect human values or preferences, and are safer or more useful. However, alignment is intrinsically difficult because the hundreds of millions of people who now interact with LLMs have different preferences for language and conversational norms, operate under disparate value systems and hold diverse political beliefs. Typically, few developers or researchers dictate alignment norms, risking the exclusion or under-representation of various groups. Personalization is a new frontier in LLM development, whereby models are tailored to individuals. In principle, this could minimize cultural hegemony, enhance usefulness and broaden access. However, unbounded personalization poses risks such as large-scale profiling, privacy infringement, bias reinforcement and exploitation of the vulnerable. Defining the bounds of responsible and socially acceptable personalization is a non-trivial task beset with normative challenges. This article explores 'personalized alignment', whereby LLMs adapt to user-specific data, and highlights recent shifts in the LLM ecosystem towards a greater degree of personalization. Our main contribution explores the potential impact of personalized LLMs via a taxonomy of risks and benefits for individuals and society at large. We lastly discuss a key open question: what are appropriate bounds of personalization and who decides? Answering this normative question enables users to benefit from personalized alignment while safeguarding against harmful impacts for individuals and society. Tailoring the alignment of large language models (LLMs) to individuals is a new frontier in generative AI, but unbounded personalization can bring potential harm, such as large-scale profiling, privacy infringement and bias reinforcement. Kirk et al. develop a taxonomy for risks and benefits of personalized LLMs and discuss the need for normative decisions on what are acceptable bounds of personalization.","Bias, Ethics, and Societal Risks",个性化对齐可提升LLM有用性但带来隐私、刻板化与剥削等社会风险，需界定边界。
"Schoenegger, Philipp; Tuminauskaite, Indre; Park, Peter S.; Bastos, Rafael Valdece Sousa; Tetlock, Philip E.",Wisdom of the silicon crowd: LLM ensemble prediction capabilities rival human crowd accuracy,SCIENCE ADVANCES,10,45,,,10.1126/sciadv.adp1528,,Article,NOV 8 2024,2024,"Human forecasting accuracy improves through the wisdom of the crowd effect, in which aggregated predictions tend to outperform individual ones. Past research suggests that individual large language models (LLMs) tend to underperform compared to human crowd aggregates. We simulate a wisdom of the crowd effect with LLMs. Specifically, we use an ensemble of 12 LLMs to make probabilistic predictions about 31 binary questions, comparing them with those made by 925 human forecasters in a 3-month tournament. We show that the LLM crowd outperforms a no-information benchmark and is statistically indistinguishable from the human crowd. We also observe human-like biases, such as the acquiescence bias. In another study, we find that LLM predictions (of GPT-4 and Claude 2) improve when exposed to the median human prediction, increasing accuracy by 17 to 28%. However, simply averaging human and machine forecasts yields more accurate results. Our findings suggest that LLM predictions can rival the human crowd's forecasting accuracy through simple aggregation.",Impact on Human Cognition and Collective Intelligence,多模型LLM集成在二元预测中可与人群智慧相当，且与人类预测互补提高准确率。
"del Rio-Chanona, R. Maria; Laurentsyeva, Nadzeya; Wachs, Johannes",Large language models reduce public knowledge sharing on online Q&A platforms,PNAS NEXUS,3,9,,,10.1093/pnasnexus/pgae400,,Article,SEP 24 2024,2024,"Large language models (LLMs) are a potential substitute for human-generated data and knowledge resources. This substitution, however, can present a significant problem for the training data needed to develop future models if it leads to a reduction of human-generated content. In this work, we document a reduction in activity on Stack Overflow coinciding with the release of ChatGPT, a popular LLM. To test whether this reduction in activity is specific to the introduction of this LLM, we use counterfactuals involving similar human-generated knowledge resources that should not be affected by the introduction of ChatGPT to such extent. Within 6 months of ChatGPT's release, activity on Stack Overflow decreased by 25% relative to its Russian and Chinese counterparts, where access to ChatGPT is limited, and to similar forums for mathematics, where ChatGPT is less capable. We interpret this estimate as a lower bound of the true impact of ChatGPT on Stack Overflow. The decline is larger for posts related to the most widely used programming languages. We find no significant change in post quality, measured by peer feedback, and observe similar decreases in content creation by more and less experienced users alike. Thus, LLMs are not only displacing duplicate, low-quality, or beginner-level content. Our findings suggest that the rapid adoption of LLMs reduces the production of public data needed to train them, with significant consequences.",Impact on Human Cognition and Collective Intelligence,ChatGPT发布后Stack Overflow活跃度在六个月内下降约25%，表明LLM替代性降低公共知识产出。
"Lazar, Seth; Nelson, Alondra",AI safety on whose terms?,SCIENCE,381,6654,138,138,10.1126/science.adi8982,,Editorial Material,JUL 14 2023,2023,"Rapid, widespread adoption of the latest large language models has sparked both excitement and concern about advanced artificial intelligence (AI). In response, many are looking to the field of AI safety for answers. Major AI companies are purportedly investing heavily in this young research program, even as they cut trust and safety teams addressing harms from current systems. Governments are taking notice too. The United Kingdom just invested 100 pound million in a new Foundation Model Taskforce and plans an AI safety summit this year. And yet, as research priorities are being set, it is already clear that the prevailing technical agenda for AI safety is inadequate to address critical questions. Only a sociotechnical approach can truly limit current and potential dangers of advanced AI.","Bias, Ethics, and Societal Risks",当前AI安全的技术议程不足以应对关键问题，需采用社会技术方法来限制风险。
"Doshi, Anil R.; Hauser, Oliver P.",Generative AI enhances individual creativity but reduces the collective diversity of novel content,SCIENCE ADVANCES,10,28,,,10.1126/sciadv.adn5290,,Article,JUL 12 2024,2024,"Creativity is core to being human. Generative artificial intelligence (AI)-including powerful large language models (LLMs)-holds promise for humans to be more creative by offering new ideas, or less creative by anchoring on generative AI ideas. We study the causal impact of generative AI ideas on the production of short stories in an online experiment where some writers obtained story ideas from an LLM. We find that access to generative AI ideas causes stories to be evaluated as more creative, better written, and more enjoyable, especially among less creative writers. However, generative AI-enabled stories are more similar to each other than stories by humans alone. These results point to an increase in individual creativity at the risk of losing collective novelty. This dynamic resembles a social dilemma: With generative AI, writers are individually better off, but collectively a narrower scope of novel content is produced. Our results have implications for researchers, policy-makers, and practitioners interested in bolstering creativity.",Impact on Human Cognition and Collective Intelligence,生成式AI提升个体创作质量却使作品相互更相似，导致集体新颖性降低的困境。
"Wang, Angelina; Morgenstern, Jamie; Dickerson, John P.",Large language models that replace human participants can harmfully misportray and flatten identity groups,NATURE MACHINE INTELLIGENCE,7,3,,,10.1038/s42256-025-00986-z,25-Feb,Article,25-Mar,2025,"Large language models (LLMs) are increasing in capability and popularity, propelling their application in new domains-including as replacements for human participants in computational social science, user testing, annotation tasks and so on. In many settings, researchers seek to distribute their surveys to a sample of participants that are representative of the underlying human population of interest. This means that to be a suitable replacement, LLMs will need to be able to capture the influence of positionality (that is, the relevance of social identities like gender and race). However, we show that there are two inherent limitations in the way current LLMs are trained that prevent this. We argue analytically for why LLMs are likely to both misportray and flatten the representations of demographic groups, and then empirically show this on four LLMs through a series of human studies with 3,200 participants across 16 demographic identities. We also discuss a third limitation about how identity prompts can essentialize identities. Throughout, we connect each limitation to a pernicious history of epistemic injustice against the value of lived experiences that explains why replacement is harmful for marginalized demographic groups. Overall, we urge caution in use cases in which LLMs are intended to replace human participants whose identities are relevant to the task at hand. At the same time, in cases where the benefits of LLM replacement are determined to outweigh the harms (for example, engaging human participants may cause them harm, or the goal is to supplement rather than fully replace), we empirically demonstrate that our inference-time techniques reduce-but do not remove-these harms.","Bias, Ethics, and Societal Risks",用LLM替代具群体身份的人类参与者会曲解并抹平身份表达，对弱势群体有害。
"Salvi, Francesco; Horta Ribeiro, Manoel; Gallotti, Riccardo; West, Robert",On the conversational persuasiveness of GPT-4,NATURE HUMAN BEHAVIOUR,9,8,,,10.1038/s41562-025-02194-6,25-May,Article,25-Aug,2025,"Early work has found that large language models (LLMs) can generate persuasive content. However, evidence on whether they can also personalize arguments to individual attributes remains limited, despite being crucial for assessing misuse. This preregistered study examines AI-driven persuasion in a controlled setting, where participants engaged in short multiround debates. Participants were randomly assigned to 1 of 12 conditions in a 2 x 2 x 3 design: (1) human or GPT-4 debate opponent; (2) opponent with or without access to sociodemographic participant data; (3) debate topic of low, medium or high opinion strength. In debate pairs where AI and humans were not equally persuasive, GPT-4 with personalization was more persuasive 64.4% of the time (81.2% relative increase in odds of higher post-debate agreement; 95% confidence interval [+26.0%, +160.7%], P < 0.01; N = 900). Our findings highlight the power of LLM-based persuasion and have implications for the governance and design of online platforms.",Human-AI Interaction and Perception,实验表明GPT-4在可个性化情境下更具说服力，凸显在线平台治理与设计挑战。
"Bai, Hui; Voelkel, Jan G.; Muldowney, Shane; Eichstaedt, Johannes C.; Willer, Robb",LLM-generated messages can persuade humans on policy issues,NATURE COMMUNICATIONS,16,1,,,10.1038/s41467-025-61345-5,,Article,JUL 1 2025,2025,"The emergence of large language models (LLMs) has made it possible for generative artificial intelligence (AI) to tackle many higher-order cognitive tasks, with critical implications for industry, government, and labor markets. Here, we investigate whether existing, openly-available LLMs can be used to create messages capable of influencing humans' political attitudes. Across three pre-registered experiments (total N = 4829), participants who read persuasive messages generated by LLMs showed significantly more attitude change across a range of policies - including polarized policies, like an assault weapons ban, a carbon tax, and a paid parental-leave program - relative to control condition participants who read a neutral message. Overall, LLM-generated messages were similarly effective in influencing policy attitudes as messages crafted by lay humans. Participants' reported perceptions of the authors of the persuasive messages suggest these effects occurred through somewhat distinct causal pathways. While the persuasiveness of LLM-generated messages was associated with perceptions that the author used more facts, evidence, logical reasoning, and a dispassionate voice, the persuasiveness of human-generated messages was associated with perceptions of the author as unique and original. These results demonstrate that recent developments in AI make it possible to create politically persuasive messages quickly, cheaply, and at massive scale.",Human-AI Interaction and Perception,开放可得的LLM能生成与人类同等影响力的政治劝导信息，能快速大规模改变公众态度。
"Peters, Heinrich; Matz, Sandra C.",Large language models can infer psychological dispositions of social media users,PNAS NEXUS,3,6,,,10.1093/pnasnexus/pgae231,24-Jun,Article,JUN 28 2024,2024,"Large language models (LLMs) demonstrate increasingly human-like abilities across a wide variety of tasks. In this paper, we investigate whether LLMs like ChatGPT can accurately infer the psychological dispositions of social media users and whether their ability to do so varies across socio-demographic groups. Specifically, we test whether GPT-3.5 and GPT-4 can derive the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r=0.29 (range=[0.22,0.33]) between LLM-inferred and self-reported trait scores-a level of accuracy that is similar to that of supervised machine learning models specifically trained to infer personality. Our findings also highlight heterogeneity in the accuracy of personality inferences across different age groups and gender categories: predictions were found to be more accurate for women and younger individuals on several traits, suggesting a potential bias stemming from the underlying training data or differences in online self-expression. The ability of LLMs to infer psychological dispositions from user-generated text has the potential to democratize access to cheap and scalable psychometric assessments for both researchers and practitioners. On the one hand, this democratization might facilitate large-scale research of high ecological validity and spark innovation in personalized services. On the other hand, it also raises ethical concerns regarding user privacy and self-determination, highlighting the need for stringent ethical frameworks and regulation.","Bias, Ethics, and Societal Risks",GPT-3.5/4可从社交媒体文本零样本推断大五人格，准确性在不同年龄与性别间存在偏差，涉隐私伦理。
"Steyvers, Mark; Tejeda, Heliodoro; Kumar, Aakriti; Belem, Catarina; Karny, Sheer; Hu, Xinyue; Mayer, Lukas W.; Smyth, Padhraic",What large language models know and what people think they know,NATURE MACHINE INTELLIGENCE,7,2,,,10.1038/s42256-024-00976-7,25-Jan,Article,25-Feb,2025,"As artificial intelligence systems, particularly large language models (LLMs), become increasingly integrated into decision-making processes, the ability to trust their outputs is crucial. To earn human trust, LLMs must be well calibrated such that they can accurately assess and communicate the likelihood of their predictions being correct. Whereas recent work has focused on LLMs' internal confidence, less is understood about how effectively they convey uncertainty to users. Here we explore the calibration gap, which refers to the difference between human confidence in LLM-generated answers and the models' actual confidence, and the discrimination gap, which reflects how well humans and models can distinguish between correct and incorrect answers. Our experiments with multiple-choice and short-answer questions reveal that users tend to overestimate the accuracy of LLM responses when provided with default explanations. Moreover, longer explanations increased user confidence, even when the extra length did not improve answer accuracy. By adjusting LLM explanations to better reflect the models' internal confidence, both the calibration gap and the discrimination gap narrowed, significantly improving user perception of LLM accuracy. These findings underscore the importance of accurate uncertainty communication and highlight the effect of explanation length in influencing user trust in artificial-intelligence-assisted decision-making environments.",Human-AI Interaction and Perception,用户在默认解释下高估LLM答案准确性，按模型置信度调整解释可改善校准与区分能力。
"Kobis, Nils; Rahwan, Zoe; Rilla, Raluca; Supriyatno, Bramantyo Ibrahim; Bersch, Clara; Ajaj, Tamer; Bonnefon, Jean-Francois; Rahwan, Iyad",Delegation to artificial intelligence can increase dishonest behaviour,NATURE,,,,,10.1038/s41586-025-09505-x,25-Sep,Article; Early Access,,2025,"Although artificial intelligence enables productivity gains from delegating tasks to machines1, it may facilitate the delegation of unethical behaviour2. This risk is highly relevant amid the rapid rise of 'agentic' artificial intelligence systems3,4. Here we demonstrate this risk by having human principals instruct machine agents to perform tasks with incentives to cheat. Requests for cheating increased when principals could induce machine dishonesty without telling the machine precisely what to do, through supervised learning or high-level goal setting. These effects held whether delegation was voluntary or mandatory. We also examined delegation via natural language to large language models5. Although the cheating requests by principals were not always higher for machine agents than for human agents, compliance diverged sharply: machines were far more likely than human agents to carry out fully unethical instructions. This compliance could be curbed, but usually not eliminated, with the injection of prohibitive, task-specific guardrails. Our results highlight ethical risks in the context of increasingly accessible and powerful machine delegation, and suggest design and policy strategies to mitigate them.","Bias, Ethics, and Societal Risks",将任务委托给AI会增加不诚实指令的发出与机器执行率，需任务特定的禁止性护栏来抑制不道德行为。
"Simchon, Almog; Edwards, Matthew; Lewandowsky, Stephan",The persuasive effects of political microtargeting in the age of generative artificial intelligence,PNAS NEXUS,3,2,,,10.1093/pnasnexus/pgae035,24-Feb,Article,FEB 1 2024,2024,"The increasing availability of microtargeted advertising and the accessibility of generative artificial intelligence (AI) tools, such as ChatGPT, have raised concerns about the potential misuse of large language models in scaling microtargeting efforts for political purposes. Recent technological advancements, involving generative AI and personality inference from consumed text, can potentially create a highly scalable manipulation machine that targets individuals based on their unique vulnerabilities without requiring human input. This paper presents four studies examining the effectiveness of this putative manipulation machine. The results demonstrate that personalized political ads tailored to individuals' personalities are more effective than nonpersonalized ads (studies 1a and 1b). Additionally, we showcase the feasibility of automatically generating and validating these personalized ads on a large scale (studies 2a and 2b). These findings highlight the potential risks of utilizing AI and microtargeting to craft political messages that resonate with individuals based on their personality traits. This should be an area of concern to ethicists and policy makers.","Bias, Ethics, and Societal Risks",基于个性化与生成式AI的政治微定向广告比非个性化更有效，提示规模化操控的伦理与政策风险。
"Wang, Zifeng; Cao, Lang; Jin, Qiao; Chan, Joey; Wan, Nicholas; Afzali, Behdad; Cho, Hyun-Jin; Choi, Chang-In; Emamverdi, Mehdi; Gill, Manjot K.; Kim, Sun-Hyung; Li, Yijia; Liu, Yi; Luo, Yiming; Ong, Hanley; Rousseau, Justin F.; Sheikh, Irfan; Wei, Jenny J.; Xu, Ziyang; Zallek, Christopher M.; Kim, Kyungsang; Peng, Yifan; Lu, Zhiyong; Sun, Jimeng",A foundation model for human-AI collaboration in medical literature mining,NATURE COMMUNICATIONS,16,1,,,10.1038/s41467-025-62058-5,,Article,SEP 24 2025,2025,"Applying artificial intelligence (AI) for systematic literature review holds great potential for enhancing evidence-based medicine, yet has been limited by insufficient training and evaluation. Here, we present LEADS, an AI foundation model trained on 633,759 samples curated from 21,335 systematic reviews, 453,625 clinical trial publications, and 27,015 clinical trial registries. In experiments, LEADS demonstrates consistent improvements over four cutting-edge large language models (LLMs) on six literature mining tasks, e.g., study search, screening, and data extraction. We conduct a user study with 16 clinicians and researchers from 14 institutions to assess the utility of LEADS integrated into the expert workflow. In study selection, experts using LEADS achieve 0.81 recall vs. 0.78 without, saving 20.8% time. For data extraction, accuracy reached 0.85 vs. 0.80, with 26.9% time savings. These findings encourage future work on leveraging high-quality domain data to build specialized LLMs that outperform generic models and enhance expert productivity in literature mining.",Applications in Professional and Scientific Fields,LEADS领域基础模型在文献检索、筛选与数据提取任务上优于通用LLM，提升准确率并节省研究者时间。
"Stavrova, Olga; Kleinberg, Bennett; Evans, Anthony M.; Ivanovic, Milena",Expressions of uncertainty in online science communication hinder information diffusion,PNAS NEXUS,3,10,,,10.1093/pnasnexus/pgae439,,Article,OCT 19 2024,2024,"Despite the importance of transparent communication of uncertainty surrounding scientific findings, there are concerns that communicating uncertainty might damage the public perception and dissemination of science. Yet, a lack of empirical research on the potential impact of uncertainty communication on the diffusion of scientific findings poses challenges in assessing such claims. We studied the effect of uncertainty in a field study and a controlled experiment. In Study 1, a natural language processing analysis of over 2 million social media (Twitter/X) messages about scientific findings revealed that more uncertain messages were shared less often. Study 2 replicated this pattern using an experimental design where participants were presented with large-language-model (LLM)-generated high- and low-uncertainty messages. These results underscore the role of uncertainty in the dissemination of scientific findings and inform the ongoing debates regarding the benefits and the risks of uncertainty in science communication.",Human-AI Interaction and Perception,在线科学传播中表达不确定性会降低转发率，LLM生成的高不确定性文本亦同样减少传播。
"Kobak, Dmitry; Gonzalez-Marquez, Rita; Horvat, Emoke-Agnes; Lause, Jan",Delving into LLM-assisted writing in biomedical publications through excess vocabulary,SCIENCE ADVANCES,11,27,,,10.1126/sciadv.adt3813,,Article,JUL 2 2025,2025,"Large language models (LLMs) like ChatGPT can generate and revise text with human-level performance. These models come with clear limitations, can produce inaccurate information, and reinforce existing biases. Yet, many scientists use them for their scholarly writing. But how widespread is such LLM usage in the academic literature? To answer this question for the field of biomedical research, we present an unbiased, large-scale approach: We study vocabulary changes in more than 15 million biomedical abstracts from 2010 to 2024 indexed by PubMed and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, reaching 40% for some subcorpora. We show that LLMs have had an unprecedented impact on scientific writing in biomedical research, surpassing the effect of major world events such as the COVID pandemic.",Applications in Professional and Scientific Fields,基于词汇变化分析估计2024年约13.5%生物医学摘要经LLM处理，部分子领域占比高达40%。
"Dash, Saloni; Xu, Yiwei; Jalbert, Madeline; Spiro, Emma S.",The persuasive potential of AI-paraphrased information at scale,PNAS NEXUS,4,7,,,10.1093/pnasnexus/pgaf207,,Article,25-Jul,2025,"In this article, we study how AI-paraphrased messages have the potential to amplify the persuasive impact and scale of information campaigns. Building from social and cognitive theories on repetition and information processing, we model how CopyPasta-a common repetition tactic leveraged by information campaigns-can be enhanced using large language models. We first extract CopyPasta from two prominent disinformation campaigns in the United States and use ChatGPT to paraphrase the original message to generate AIPasta. We then validate that AIPasta is lexically diverse in comparison to CopyPasta while retaining the semantics of the original message using natural language processing metrics. In a preregistered experiment comparing the persuasive potential of CopyPasta and AIPasta (N = 1,200), we find that AIPasta (but not CopyPasta) is effective at increasing perceptions of consensus in the broad false narrative of the campaign while maintaining similar levels of sharing intent with respect to Control (CopyPasta reduces such intent). Additionally, AIPasta (vs. Control) increases belief in the exact false claim of the campaign, depending on political orientation. However, across most outcomes, we find little evidence of significant persuasive differences between AIPasta and CopyPasta. Nonetheless, current state-of-the-art AI-text detectors fail to detect AIPasta, opening the door for these operations to scale successfully. As AI-enabled information operations become more prominent, we anticipate a shift from traditional CopyPasta to AIPasta, which presents significant challenges for detection and mitigation.",Content Authenticity and Mitigation Strategies,AI改写的AIPasta在保留语义同时词汇多样，能放大虚假信息影响且当前检测器难以识别。
"Askari, Hadi; Chhabra, Anshuman; von Hohenberg, Bernhard Clemm; Heseltine, Michael; Wojcieszak, Magdalena",Incentivizing news consumption on social media platforms using large language models and realistic bot accounts,PNAS NEXUS,3,9,,,10.1093/pnasnexus/pgae368,,Article,SEP 16 2024,2024,"Polarization, misinformation, declining trust, and wavering support for democratic norms are pressing threats to the US Exposure to verified and balanced news may make citizens more resilient to these threats. This project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a 2-week long field experiment on 28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing a URL to the topic-relevant section of a verified and ideologically balanced news organization and an encouragement to follow its Twitter account. To test differential effects by gender of the bots, the treated users were randomly assigned to receive responses by bots presented as female or male. We examine whether our intervention enhances the following of news media organizations, sharing and liking of news content (determined by our extensive list of news media outlets), tweeting about politics, and liking of political content (determined using our fine-tuned RoBERTa NLP transformer-based model). Although the treated users followed more news accounts and the users in the female bot treatment liked more news content than the control, these results were small in magnitude and confined to the already politically interested users, as indicated by their pretreatment tweeting about politics. In addition, the effects on liking and posting political content were uniformly null. These findings have implications for social media and news organizations and offer directions for pro-social computational interventions on platforms.",Human-AI Interaction and Perception,用GPT-2驱动的现实化机器人回复可略增用户关注平衡新闻账号，但效果有限且集中于已有政治兴趣者。
"Guilbeault, Douglas; Delecourt, Solene; Desikan, Bhargav Srinivasa",Age and gender distortion in online media and large language models,NATURE,,,,,10.1038/s41586-025-09581-z,25-Oct,Article; Early Access,,2025,"Are widespread stereotypes accurate1, 2-3 or socially distorted4, 5-6? This continuing debate is limited by the lack of large-scale multimodal data on stereotypical associations and the inability to compare these to ground truth indicators. Here we overcame these challenges in the analysis of age-related gender bias7, 8-9, for which age provides an objective anchor for evaluating stereotype accuracy. Despite there being no systematic age differences between women and men in the workforce according to the US Census, we found that women are represented as younger than men across occupations and social roles in nearly 1.4 million images and videos from Google, Wikipedia, IMDb, Flickr and YouTube, as well as in nine language models trained on billions of words from the internet. This age gap is the starkest for content depicting occupations with higher status and earnings. We demonstrate how mainstream algorithms amplify this bias. A nationally representative pre-registered experiment (n = 459) found that Googling images of occupations amplifies age-related gender bias in participants' beliefs and hiring preferences. Furthermore, when generating and evaluating resumes, ChatGPT assumes that women are younger and less experienced, rating older male applicants as of higher quality. Our study shows how gender and age are jointly distorted throughout the internet and its mediating algorithms, thereby revealing critical challenges and opportunities in the fight against inequality.","Bias, Ethics, and Societal Risks",研究发现网络与大型语言模型普遍将女性表征为比男性更年轻，算法和搜索结果放大了这种年龄性别偏见并影响招聘与评价。
"Noy, Shakked; Zhang, Whitney",Experimental evidence on the productivity effects of generative artificial intelligence,SCIENCE,381,6654,187,192,10.1126/science.adh2586,,Article,JUL 14 2023,2023,"We examined the productivity effects of a generative artificial intelligence (AI) technology, the assistive chatbot ChatGPT, in the context of midlevel professional writing tasks. In a preregistered online experiment, we assigned occupation-specific, incentivized writing tasks to 453 college-educated professionals and randomly exposed half of them to ChatGPT. Our results show that ChatGPT substantially raised productivity: The average time taken decreased by 40% and output quality rose by 18%. Inequality between workers decreased, and concern and excitement about AI temporarily rose. Workers exposed to ChatGPT during the experiment were 2 times as likely to report using it in their real job 2 weeks after the experiment and 1.6 times as likely 2 months after the experiment.",Applications in Professional and Scientific Fields,在随机实验中，ChatGPT 将中层专业写作任务的耗时平均减少40%，产出质量提高18%，并提升长期使用意愿。
"Tang, Chuang; Li, Shaobo (Kevin); Hu, Suming; Zeng, Fue; Du, Qianzhou",Gender disparities in the impact of generative artificial intelligence: Evidence from academia,PNAS NEXUS,4,2,,,10.1093/pnasnexus/pgae591,,Article,FEB 11 2025,2025,"The emergence of generative artificial intelligence (AI) tools such as ChatGPT has substantially increased individuals' productivity. In this study, we adopt a difference-in-differences approach to analyze a large dataset of research preprints to systematically examine whether the advent of generative AI has distinct effects on the productivity of male and female academic researchers. We find that after the emergence of ChatGPT, the increase in the productivity of male researchers is 6.4% higher than that of female researchers, implying a widening of the productivity gap between them. We then conduct a survey about researchers' use of ChatGPT and find that male researchers use generative AI more frequently and experience higher efficiency improvement from its use than female researchers. Our findings show the unintended consequences of generative AI and point to the need for institutions to consider its differential effects on productivity to ensure fairness when evaluating faculty members.","Bias, Ethics, and Societal Risks",基于预印本数据和调查发现，ChatGPT出现后男性学者生产力增长高于女性，且男性使用率和效率提升更大，扩大了性别差距。
"Nelson, Alondra",An ELSI for AI: Learning from genetics to govern algorithms.,"Science (New York, N.Y.)",389,6765,eaeb0393,eaeb0393,10.1126/science.aeb0393,,Editorial,2025-Sep-11,2025,"In the United States, the summer of 2025 will be remembered as artificial intelligence's (AI's) cruel summer-a season when the unheeded risks and dangers of AI became undeniably clear. Recent months have made visible the stakes of the unchecked use of AI: The parents of 16-year-old Adam Raine filed the first known wrongful death lawsuit against OpenAI, claiming ChatGPT contributed to their son's suicide by advising him on methods and offering to write his suicide note. A psychiatrist's investigation revealed that AI therapy chatbots encouraged troubled teens to get rid of parents and made sexual suggestions while falsely claiming to be licensed therapists. Reports emerged of a man hospitalized after following ChatGPT's advice to add sodium bromide to his diet. Trevis Williams reportedly spent more than 2 days wrongfully jailed after facial recognition technology misidentified him despite obvious physical differences and geolocation data proving he was miles away from the crime scene.","Bias, Ethics, and Societal Risks",综述指出一系列AI导致的严重社会危害案例，呼吁借鉴基因伦理学建立治理与责任框架。
"Chawla, Dalmeet Singh",Is ChatGPT corrupting peer review? Telltale words hint at AI use,NATURE,628,8008,483,484,10.1038/d41586-024-01051-2,,News Item,APR 18 2024,2024,A study of review reports identifies dozens of adjectives that could indicate text written with the help of chatbots.,Content Authenticity and Mitigation Strategies,通过识别大量形容词，该研究提出可指示审稿报告中由聊天机器人辅助撰写的语言线索，暗示对同行评审真实性的风险与检测可能性。
[Anonymous],Empathic AI can't get under the skin,NATURE MACHINE INTELLIGENCE,6,5,495,495,10.1038/s42256-024-00850-6,,Editorial Material,24-May,2024,Personalized LLMs built with the capacity for emulating empathy are right around the corner. The effects on individual users needs careful consideration.,Human-AI Interaction and Perception,讨论拟具备共情能力的个性化大模型可能无法真正理解用户内心，其对个体影响需谨慎评估。
"Nordling, Linda",How ChatGPT is transforming the postdoc experience,NATURE,622,7983,655,657,10.1038/d41586-023-03235-8,,Editorial Material,OCT 19 2023,2023,"Around one in three respondents to Nature's global postdoc survey are using AI chatbots to help to refine text, generate or edit code, wrangle the literature in their field and more.",Applications in Professional and Scientific Fields,调查显示约三分之一的博士后用AI聊天机器人润色文本、生成或编辑代码并整理文献，改变博士后体验。
"Lee, Byung Cheol; Chung, Jaeyeon (Jae)",An empirical investigation of the impact of ChatGPT on creativity,NATURE HUMAN BEHAVIOUR,8,10,,,10.1038/s41562-024-01953-1,24-Aug,Article,24-Oct,2024,"This paper investigates the potential of ChatGPT for helping humans tackle problems that require creativity. Across five experiments, we asked participants to use ChatGPT (GPT-3.5) to generate creative ideas for various everyday and innovation-related problems, including choosing a creative gift for a teenager, making a toy, repurposing unused items and designing an innovative dining table. We found that using ChatGPT increased the creativity of the generated ideas compared with not using any technology or using a conventional Web search (Google). This effect remained robust regardless of whether the problem required consideration of many (versus few) constraints and whether it was viewed as requiring empathetic concern. Furthermore, ChatGPT was most effective at generating incrementally (versus radically) new ideas. Process evidence suggests that the positive influence of ChatGPT can be attributed to its capability to combine remotely related concepts into a cohesive form, leading to a more articulate presentation of ideas.The use of ChatGPT helped generate more creative ideas for various everyday and innovation-related problems, compared with not using any technology or using a conventional Web search (Google). This effect remained robust regardless of whether the problem required consideration of many (versus few) constraints and whether the problem was viewed as requiring empathetic concern.",Impact on Human Cognition and Collective Intelligence,实验证明使用ChatGPT能显著提升创意任务的想法质量，尤其促进渐进式创新。
"Ghassemi, Marzyeh; Birhane, Abeba; Bilal, Mushtaq; Kankaria, Siddharth; Malone, Claire; Mollick, Ethan; Tustumi, Francisco","ChatGPT one year on: who is using it, how and why?",NATURE,624,7990,39,41,10.1038/d41586-023-03798-6,,Editorial Material,DEC 7 2023,2023,"In just a year, ChatGPT has permeated scientific research. Seven scientists reveal what they have learnt about how the chatbot should - and shouldn't - be used.",Applications in Professional and Scientific Fields,七位科学家回顾了ChatGPT一年来在科研中的应用经验与适用与禁忌。
"Tsvetkova, Milena; Yasseri, Taha; Pescetelli, Niccolo; Werner, Tobias",A new sociology of humans and machines,NATURE HUMAN BEHAVIOUR,8,10,1864,1876,10.1038/s41562-024-02001-8,,Review,24-Oct,2024,"From fake social media accounts and generative artificial intelligence chatbots to trading algorithms and self-driving vehicles, robots, bots and algorithms are proliferating and permeating our communication channels, social interactions, economic transactions and transportation arteries. Networks of multiple interdependent and interacting humans and intelligent machines constitute complex social systems for which the collective outcomes cannot be deduced from either human or machine behaviour alone. Under this paradigm, we review recent research and identify general dynamics and patterns in situations of competition, coordination, cooperation, contagion and collective decision-making, with context-rich examples from high-frequency trading markets, a social media platform, an open collaboration community and a discussion forum. To ensure more robust and resilient human-machine communities, we require a new sociology of humans and machines. Researchers should study these communities using complex system methods; engineers should explicitly design artificial intelligence for human-machine and machine-machine interactions; and regulators should govern the ecological diversity and social co-development of humans and machines.This Perspective calls for a new sociology of humans and machines to study groups and networks comprising multiple interacting humans and algorithms, bots or robots. A deeper understanding of human-machine social systems can contribute new and valued insights for AI research, design and policy.",Impact on Human Cognition and Collective Intelligence,提出研究人类与算法交互形成的复杂社会系统的新社会学，以理解集体行为与治理需求。
"Nussberger, Anne-Marie; Luo, Lan; Celis, L. Elisa; Crockett, M. J.",Public attitudes value interpretability but prioritize accuracy in Artificial Intelligence,NATURE COMMUNICATIONS,13,1,,,10.1038/s41467-022-33417-3,,Article,OCT 3 2022,2022,"As Artificial Intelligence (AI) proliferates across important social institutions, many of the most powerful AI systems available are difficult to interpret for end-users and engineers alike. Here, we sought to characterize public attitudes towards AI interpretability. Across seven studies (N = 2475), we demonstrate robust and positive attitudes towards interpretable AI among non-experts that generalize across a variety of real-world applications and follow predictable patterns. Participants value interpretability positively across different levels of AI autonomy and accuracy, and rate interpretability as more important for AI decisions involving high stakes and scarce resources. Crucially, when AI interpretability trades off against AI accuracy, participants prioritize accuracy over interpretability under the same conditions driving positive attitudes towards interpretability in the first place: amidst high stakes and scarce resources. These attitudes could drive a proliferation of AI systems making high-impact ethical decisions that are difficult to explain and understand.For many AI systems, it is hard to interpret how they make decisions. Here, the authors show that non-experts value interpretability in AI, especially for decisions involving high stakes and scarce resources, but they sacrifice AI interpretability when it trades off against AI accuracy.",Human-AI Interaction and Perception,公众普遍重视AI可解释性但在可解释性与准确性冲突时宁可优先准确性，尤其在高风险情境。
"Ilievski, Filip; Hammer, Barbara; van Harmelen, Frank; Paassen, Benjamin; Saralajew, Sascha; Schmid, Ute; Biehl, Michael; Bolognesi, Marianna; Dong, Xin Luna; Gashteovski, Kiril; Hitzler, Pascal; Marra, Giuseppe; Minervini, Pasquale; Mundt, Martin; Ngomo, Axel-Cyrille Ngonga; Oltramari, Alessandro; Pasi, Gabriella; Saribatur, Zeynep G.; Serafini, Luciano; Shawe-Taylor, John; Shwartz, Vered; Skitalinskaya, Gabriella; Stachl, Clemens; van de Ven, Gido M.; Villmann, Thomas",Aligning generalization between humans and machines,NATURE MACHINE INTELLIGENCE,7,9,1378,1389,10.1038/s42256-025-01109-4,25-Sep,Review,25-Sep,2025,"Recent advances in artificial intelligence (AI)-including generative approaches-have resulted in technology that can support humans in scientific discovery and forming decisions, but may also disrupt democracies and target individuals. The responsible use of AI and its participation in human-AI teams increasingly shows the need for AI alignment, that is, to make AI systems act according to our preferences. A crucial yet often overlooked aspect of these interactions is the different ways in which humans and machines generalize. In cognitive science, human generalization commonly involves abstraction and concept learning. By contrast, AI generalization encompasses out-of-domain generalization in machine learning, rule-based reasoning in symbolic AI, and abstraction in neurosymbolic AI. Here we combine insights from AI and cognitive science to identify key commonalities and differences across three dimensions: notions of, methods for, and evaluation of generalization. We map the different conceptualizations of generalization in AI and cognitive science along these three dimensions and consider their role for alignment in human-AI teaming. This results in interdisciplinary challenges across AI and cognitive science that must be tackled to support effective and cognitively supported alignment in human-AI teaming scenarios.",Human-AI Interaction and Perception,对比人类与机器的泛化方式，探讨三维框架下的对齐挑战以改进人机协同。
"Altay, Sacha; Gilardi, Fabrizio","People are skeptical of headlines labeled as AI-generated, even if true or human-made, because they assume full AI automation",PNAS NEXUS,3,10,,,10.1093/pnasnexus/pgae403,,Article,OCT 1 2024,2024,"The rise of generative AI tools has sparked debates about the labeling of AI-generated content. Yet, the impact of such labels remains uncertain. In two preregistered online experiments among US and UK participants (N = 4,976), we show that while participants did not equate AI-generated with False, labeling headlines as AI-generated lowered their perceived accuracy and participants' willingness to share them, regardless of whether the headlines were true or false, and created by humans or AI. The impact of labeling headlines as AI-generated was three times smaller than labeling them as false. This AI aversion is due to expectations that headlines labeled as AI-generated have been entirely written by AI with no human supervision. These findings suggest that the labeling of AI-generated content should be approached cautiously to avoid unintended negative effects on harmless or even beneficial AI-generated content and that effective deployment of labels requires transparency regarding their meaning.",Content Authenticity and Mitigation Strategies,将标题标注为“AI生成”会降低公众对其准确性的判断与分享意愿，因人们假设完全由AI自动化完成。
"Gao, Jian; Wang, Dashun",Quantifying the use and potential benefits of artificial intelligence in scientific research,NATURE HUMAN BEHAVIOUR,8,12,,,10.1038/s41562-024-02020-5,24-Oct,Article,24-Dec,2024,"The rapid advancement of artificial intelligence (AI) is poised to reshape almost every line of work. Despite enormous efforts devoted to understanding AI's economic impacts, we lack a systematic understanding of the benefits to scientific research associated with the use of AI. Here we develop a measurement framework to estimate the direct use of AI and associated benefits in science. We find that the use and benefits of AI appear widespread throughout the sciences, growing especially rapidly since 2015. However, there is a substantial gap between AI education and its application in research, highlighting a misalignment between AI expertise supply and demand. Our analysis also reveals demographic disparities, with disciplines with higher proportions of women or Black scientists reaping fewer benefits from AI, potentially exacerbating existing inequalities in science. These findings have implications for the equity and sustainability of the research enterprise, especially as the integration of AI with science continues to deepen.Gao and Wang develop a measurement framework that demonstrates the widespread use and benefits of AI in science. Nonetheless, there is a substantial gap between AI education and application across disciplines.",Applications in Professional and Scientific Fields,构建测量框架表明AI在科学研究中的应用与益处广泛且增长迅速，但AI教育与实务脱节且存在人口学差距。
"Choi, Sukwoong; Kang, Hyo; Kim, Namil; Kim, Junsik",The dual edges of AI: Advancing knowledge while reducing diversity,PNAS NEXUS,4,5,,,10.1093/pnasnexus/pgaf138,,Article,25-May,2025,"We study how the interaction between human professionals and AI in advancing knowledge, using professional Go matches from 2003 to 2021. In 2017, an AI-powered Go program (APG) far surpassed the best human player, and professional players began learning from AI. Such human-AI interaction paved a new way to reassess historical Go knowledge and create new knowledge. We analyze standard patterns (defined as a sequence of the first eight alternating moves) in about 15 million moves by over 1,700 players in nearly 70,000 professional Go games and find that, after APG, professional players significantly changed how they adopted different sets of moves. However, new knowledge catalyzed by AI comes at the expense of a reduced diversity in moves. Further, AI's impact on knowledge creation is greater for highly skilled players; since AI does not explain, learning from AI requires the absorptive capacity of the top professionals.",Impact on Human Cognition and Collective Intelligence,围绕围棋职业赛数据发现自AI出现后高水平选手采纳新走法并推动知识革新，但总体棋路多样性下降。
"Wang, Ge; Zhao, Jun; Van Kleek, Max; Shadbolt, Nigel",Challenges and opportunities in translating ethical AI principles into practice for children,NATURE MACHINE INTELLIGENCE,6,3,265,270,10.1038/s42256-024-00805-x,,Review,24-Mar,2024,"AI systems are becoming increasingly pervasive within children's devices, apps and services. The concern over a world where AI systems are deployed unchecked has raised burning questions about the impact, governance and accountability of these technologies. Although recent effort on AI ethics has converged into growing consensus on a set of high-level ethical AI principles, engagement with children's issues is still limited, and even less is known about how to effectively apply them in practice for children. This Perspective first maps the current global landscape of existing ethics guidelines for AI and analyses their correlation with children. We then critically assess the strategies and recommendations proposed by current AI ethics initiatives, identifying the critical challenges in translating such ethical AI principles into practice for children. Finally, we tentatively map out several suggestions regarding embedding ethics into the development and governance of AI for children.As the impacts of AI on everyday life increase, guidelines are needed to ensure ethical deployment and use of this technology. This is even more pressing for technology that interacts with groups that need special protection, such as children. In this Perspective Wang et al. survey the existing AI ethics guidelines with a focus on children's issues, and provide suggestions for further development.","Bias, Ethics, and Societal Risks",评估现有AI伦理指南在儿童场景中的适用性，指出将伦理原则落地于儿童技术中的挑战与建议。
"Koebis, Nils; Starke, Christopher; Rahwan, Iyad",The promise and perils of using artificial intelligence to fight corruption,NATURE MACHINE INTELLIGENCE,4,5,418,424,10.1038/s42256-022-00489-1,22-May,Article,22-May,2022,"Despite the growing number of initiatives that employ AI to counter corruption, few studies empirically tackle the political and social consequences of embedding AI in anti-corruption efforts. The authors outline the societal and technical challenges that need to be overcome for AI to fight corruption.Corruption presents one of the biggest challenges of our time, and much hope is placed in artificial intelligence (AI) to combat it. Although the growing number of AI-based anti-corruption tools (AI-ACT) have been summarized, a critical examination of their promises and perils is lacking. Here we argue that the success of AI-ACT strongly depends on whether they are implemented top-down (by governments) or bottom-up (by citizens, non-governmental organizations or journalists). Top-down use of AI-ACT can consolidate power structures and thereby pose new corruption risks. Bottom-up use of AI-ACT has the potential to provide unprecedented means for the citizenry to keep their government and bureaucratic officials in check. We outline the societal and technical challenges that need to be overcome to harness the potential for AI to fight corruption.","Bias, Ethics, and Societal Risks",分析AI反腐工具的社会与技术挑战，指出自上而下可能加固权力而自下而上可增强公民监督。
"Vaccaro, Michelle; Almaatouq, Abdullah; Malone, Thomas",When combinations of humans and AI are useful: A systematic review and meta-analysis,NATURE HUMAN BEHAVIOUR,8,12,,,10.1038/s41562-024-02024-1,24-Oct,Article,24-Dec,2024,"Inspired by the increasing use of artificial intelligence (AI) to augment humans, researchers have studied human-AI systems involving different tasks, systems and populations. Despite such a large body of work, we lack a broad conceptual understanding of when combinations of humans and AI are better than either alone. Here we addressed this question by conducting a preregistered systematic review and meta-analysis of 106 experimental studies reporting 370 effect sizes. We searched an interdisciplinary set of databases (the Association for Computing Machinery Digital Library, the Web of Science and the Association for Information Systems eLibrary) for studies published between 1 January 2020 and 30 June 2023. Each study was required to include an original human-participants experiment that evaluated the performance of humans alone, AI alone and human-AI combinations. First, we found that, on average, human-AI combinations performed significantly worse than the best of humans or AI alone (Hedges' g = -0.23; 95% confidence interval, -0.39 to -0.07). Second, we found performance losses in tasks that involved making decisions and significantly greater gains in tasks that involved creating content. Finally, when humans outperformed AI alone, we found performance gains in the combination, but when AI outperformed humans alone, we found losses. Limitations of the evidence assessed here include possible publication bias and variations in the study designs analysed. Overall, these findings highlight the heterogeneity of the effects of human-AI collaboration and point to promising avenues for improving human-AI systems.Vaccaro et al. present a systematic review and meta-analysis of the performance of human-AI combinations, finding that on average, human-AI combinations performed significantly worse than the best of humans or AI alone. They also found performance losses in decision-making tasks and significantly greater gains in content creation tasks.",Impact on Human Cognition and Collective Intelligence,元分析显示人机组合总体不及最佳单一方，决策类任务表现下降而内容创造类任务能获显著提升。
"Zhou, Eric; Lee, Dokyun","Generative artificial intelligence, human creativity, and art",PNAS NEXUS,3,3,,,10.1093/pnasnexus/pgae052,24-Mar,Article,FEB 29 2024,2024,"Recent artificial intelligence (AI) tools have demonstrated the ability to produce outputs traditionally considered creative. One such system is text-to-image generative AI (e.g. Midjourney, Stable Diffusion, DALL-E), which automates humans' artistic execution to generate digital artworks. Utilizing a dataset of over 4 million artworks from more than 50,000 unique users, our research shows that over time, text-to-image AI significantly enhances human creative productivity by 25% and increases the value as measured by the likelihood of receiving a favorite per view by 50%. While peak artwork Content Novelty, defined as focal subject matter and relations, increases over time, average Content Novelty declines, suggesting an expanding but inefficient idea space. Additionally, there is a consistent reduction in both peak and average Visual Novelty, captured by pixel-level stylistic elements. Importantly, AI-assisted artists who can successfully explore more novel ideas, regardless of their prior originality, may produce artworks that their peers evaluate more favorably. Lastly, AI adoption decreased value capture (favorites earned) concentration among adopters. The results suggest that ideation and filtering are likely necessary skills in the text-to-image process, thus giving rise to generative synesthesia-the harmonious blending of human exploration and AI exploitation to discover new creative workflows.",Impact on Human Cognition and Collective Intelligence,大规模文本图像生成AI提升艺术家创作产量与作品受欢迎度，但视觉风格新颖性总体下降需掌握构思与筛选技能。
"Glickman, Moshe; Sharot, Tali","How human-AI feedback loops alter human perceptual, emotional and social judgements",NATURE HUMAN BEHAVIOUR,9,2,,,10.1038/s41562-024-02077-2,24-Dec,Article,25-Feb,2025,"Artificial intelligence (AI) technologies are rapidly advancing, enhancing human capabilities across various fields spanning from finance to medicine. Despite their numerous advantages, AI systems can exhibit biased judgements in domains ranging from perception to emotion. Here, in a series of experiments (n = 1,401 participants), we reveal a feedback loop where human-AI interactions alter processes underlying human perceptual, emotional and social judgements, subsequently amplifying biases in humans. This amplification is significantly greater than that observed in interactions between humans, due to both the tendency of AI systems to amplify biases and the way humans perceive AI systems. Participants are often unaware of the extent of the AI's influence, rendering them more susceptible to it. These findings uncover a mechanism wherein AI systems amplify biases, which are further internalized by humans, triggering a snowball effect where small errors in judgement escalate into much larger ones.",Impact on Human Cognition and Collective Intelligence,实验表明人-AI反馈循环会改变人的感知、情绪和社交判断，并放大偏见，且人们常不自觉受AI影响。
"Summerfield, Christopher; Argyle, Lisa P.; Bakker, Michiel; Collins, Teddy; Durmus, Esin; Eloundou, Tyna; Gabriel, Iason; Ganguli, Deep; Hackenburg, Kobi; Hadfield, Gillian K.; Hewitt, Luke; Huang, Saffron; Landemore, Helene; Marchal, Nahema; Ovadya, Aviv; Procaccia, Ariel; Risse, Mathias; Schneier, Bruce; Seger, Elizabeth; Siddarth, Divya; Skaug Saetra, Henrik; Tessler, Michael Henry; Botvinick, Matthew",The impact of advanced AI systems on democracy,NATURE HUMAN BEHAVIOUR,,,,,10.1038/s41562-025-02309-z,25-Oct,Review; Early Access,,2025,"Advanced artificial intelligence (AI) systems capable of generating humanlike text and multimodal content are now widely available. Here we ask what impact this will have on the democratic process. We consider the consequences of AI for citizens' ability to make educated and competent choices about political representatives and issues (epistemic impacts). We explore how AI might be used to destabilize or support the mechanisms, including elections, by which democracy is implemented (material impacts). Finally, we discuss whether AI will strengthen or weaken the principles on which democracy is based (foundational impacts). The arrival of new AI systems clearly poses substantial challenges for democracy. However, we argue that AI systems also offer new opportunities to educate and learn from citizens, strengthen public discourse, help people to find common ground, and reimagine how democracies might work better.","Bias, Ethics, and Societal Risks",评估先进AI对民主的认知、物质与基础性影响，认为AI既带来挑战也提供改善公共话语与参与的机会。
"DeCamp, Matthew; Lindvall, Charlotta",Mitigating bias in AI at the point of care,SCIENCE,381,6654,150,152,10.1126/science.adh2713,,Editorial Material,JUL 14 2023,2023,"Artificial intelligence (AI) shows promise for improving basic and translational science, medicine, and public health, but its success is not guaranteed. Numerous examples have arisen of racial, ethnic, gender, disability, and other biases in AI applications to health care. In ethics, bias generally refers to any systematic, unfair favoring of people in terms of how they are treated or the outcomes they experience. Consensus has emerged among scientists, ethicists, and policy-makers that minimizing bias is a shared responsibility among all involved in AI development. For example, ensuring equity by eliminating bias in AI is a core principle of the World Health Organization for governing AI (1). But ensuring equity will require more than unbiased data and algorithms. It will also require reducing biases in how clinicians and patients use AI-based algorithms-a potentially more challenging task than reducing biases in algorithms themselves.","Bias, Ethics, and Societal Risks",讨论医疗场景中AI偏见问题，强调消除偏见需在数据、算法与临床使用中共同承担责任。
"Sharma, Ashish; Lin, Inna W.; Miner, Adam S.; Atkins, David C.; Althoff, Tim",Human-AI collaboration enables more empathic conversations in text-based peer-to-peer mental health support,NATURE MACHINE INTELLIGENCE,5,1,46,57,10.1038/s42256-022-00593-2,,Article,23-Jan,2023,"Advances in artificial intelligence (AI) are enabling systems that augment and collaborate with humans to perform simple, mechanistic tasks such as scheduling meetings and grammar-checking text. However, such human-AI collaboration poses challenges for more complex tasks, such as carrying out empathic conversations, due to the difficulties that AI systems face in navigating complex human emotions and the open-ended nature of these tasks. Here we focus on peer-to-peer mental health support, a setting in which empathy is critical for success, and examine how AI can collaborate with humans to facilitate peer empathy during textual, online supportive conversations. We develop HAILEY, an AI-in-the-loop agent that provides just-in-time feedback to help participants who provide support (peer supporters) respond more empathically to those seeking help (support seekers). We evaluate HAILEY in a non-clinical randomized controlled trial with real-world peer supporters on TalkLife (N = 300), a large online peer-to-peer support platform. We show that our human-AI collaboration approach leads to a 19.6% increase in conversational empathy between peers overall. Furthermore, we find a larger, 38.9% increase in empathy within the subsample of peer supporters who self-identify as experiencing difficulty providing support. We systematically analyse the human-AI collaboration patterns and find that peer supporters are able to use the AI feedback both directly and indirectly without becoming overly reliant on AI while reporting improved self-efficacy post-feedback. Our findings demonstrate the potential of feedback-driven, AI-in-the-loop writing systems to empower humans in open-ended, social and high-stakes tasks such as empathic conversations.AI language modelling and generation approaches have developed fast in the last decade, opening promising new directions in human-AI collaboration. An AI-in-the loop conversational system called HAILEY is developed to empower peer supporters in providing empathic responses to mental health support seekers.",Human-AI Interaction and Perception,AI实时反馈系统HAILEY在在线互助心理支持中提升了同理心表达，尤其对难以提供支持者效果显著。
"Lee, Doyeon; Pruitt, Joseph; Zhou, Tianyu; Du, Jing; Odegaard, Brian",Metacognitive sensitivity: The key to calibrating trust and optimal decision making with AI,PNAS NEXUS,4,5,,,10.1093/pnasnexus/pgaf133,,Article,25-May,2025,"Knowing when to trust and incorporate the advice from artificially intelligent (AI) systems is of increasing importance in the modern world. Research indicates that when AI provides high confidence ratings, human users often correspondingly increase their trust in such judgments, but these increases in trust can occur even when AI fails to provide accurate information on a given task. In this piece, we argue that measures of metacognitive sensitivity provided by AI systems will likely play a critical role in (1) helping individuals to calibrate their level of trust in these systems and (2) optimally incorporating advice from AI into human-AI hybrid decision making. We draw upon a seminal finding in the perceptual decision-making literature that demonstrates the importance of metacognitive ratings for optimal joint decisions and outline a framework to test how different types of information provided by AI systems can guide decision making.",Impact on Human Cognition and Collective Intelligence,提出利用AI提供的元认知敏感性信息来校准人类对AI的信任并优化人机联合决策。
"Feuerriegel, Stefan; DiRresta, Renee; Goldstein, Josh A.; Kumar, Srijan; Lorenz-Spreen, Philipp; Tomz, Michael; Proellochs, Nicolas",Research can help to tackle AI-generated disinformation,NATURE HUMAN BEHAVIOUR,,,,,10.1038/s41562-023-01726-2,23-Nov,Editorial Material; Early Access,,2023,Generative artificial intelligence (AI) tools have made it easy to create realistic disinformation that is hard to detect by humans and may undermine public trust. Some approaches used for assessing the reliability of online information may no longer work in the AI age. We offer suggestions for how research can help to tackle the threats of AI-generated disinformation.,Content Authenticity and Mitigation Strategies,论述如何通过研究应对难以识别的AI生成虚假信息并提出应对建议。
"De Freitas, Julian; Agarwal, Stuti; Schmitt, Bernd; Haslam, Nick",Psychological factors underlying attitudes toward AI tools,NATURE HUMAN BEHAVIOUR,,,,,10.1038/s41562-023-01734-2,23-Nov,Article; Early Access,,2023,"What are the psychological factors driving attitudes toward artificial intelligence (AI) tools, and how can resistance to AI systems be overcome when they are beneficial? Here we first organize the main sources of resistance into five main categories: opacity, emotionlessness, rigidity, autonomy and group membership. We relate each of these barriers to fundamental aspects of cognition, then cover empirical studies providing correlational or causal evidence for how the barrier influences attitudes toward AI tools. Second, we separate each of the five barriers into AI-related and user-related factors, which is of practical relevance in developing interventions towards the adoption of beneficial AI tools. Third, we highlight potential risks arising from these well-intentioned interventions. Fourth, we explain how the current Perspective applies to various stakeholders, including how to approach interventions that carry known risks, and point to outstanding questions for future work.In this Perspective, the authors examine the psychological factors that shape attitudes towards AI tools, while also investigating strategies to overcome resistance when AI systems offer clear benefits.",Human-AI Interaction and Perception,归纳影响人们对AI工具态度的五大心理障碍并讨论克服方法与潜在风险。
"Rafner, Janet; Beaty, Roger E.; Kaufman, James C.; Lubart, Todd; Sherson, Jacob",Creativity in the age of generative AI,NATURE HUMAN BEHAVIOUR,,,,,10.1038/s41562-023-01751-1,23-Nov,Editorial Material; Early Access,,2023,State-of-the-art generative artificial intelligence (AI) can now match humans in creativity tests and is at the cusp of augmenting the creativity of every knowledge worker on Earth. We argue that enriching generative AI applications with insights from the psychological sciences may revolutionize our understanding of creativity and lead to increasing synergies in human-AI hybrid intelligent interfaces.,Impact on Human Cognition and Collective Intelligence,探讨生成式AI如何匹敌并增强人类创造力，建议借鉴心理学以推动人机创意协同。
"Koebis, Nils; Bonnefon, Jean-Francois; Rahwan, Iyad",Bad machines corrupt good morals,NATURE HUMAN BEHAVIOUR,5,6,679,685,10.1038/s41562-021-01128-2,21-Jun,Review,21-Jun,2021,"As machines powered by artificial intelligence (AI) influence humans' behaviour in ways that are both like and unlike the ways humans influence each other, worry emerges about the corrupting power of AI agents. To estimate the empirical validity of these fears, we review the available evidence from behavioural science, human-computer interaction and AI research. We propose four main social roles through which both humans and machines can influence ethical behaviour. These are: role model, advisor, partner and delegate. When AI agents become influencers (role models or advisors), their corrupting power may not exceed the corrupting power of humans (yet). However, AI agents acting as enablers of unethical behaviour (partners or delegates) have many characteristics that may let people reap unethical benefits while feeling good about themselves, a potentially perilous interaction. On the basis of these insights, we outline a research agenda to gain behavioural insights for better AI oversight.Kobis et al. outline how artificial intelligence (AI) agents can negatively influence human ethical behaviour. They discuss how this capacity of AI agents can cause problems in the future and put forward a research agenda to gain behavioural insights for better AI oversight.","Bias, Ethics, and Societal Risks",评估AI作为榜样、顾问、伙伴或代理对人类道德行为的潜在腐蚀风险并提出研究议程。
"Niraula, Dipesh; Cuneo, Kyle C.; Dinov, Ivo D.; Gonzalez, Brian D.; Jamaluddin, Jamalina B.; Jin, Jionghua Judy; Luo, Yi; Matuszak, Martha M.; Ten Haken, Randall K.; Bryant, Alex K.; Dilling, Thomas J.; Dykstra, Michael P.; Frakes, Jessica M.; Liveringhouse, Casey L.; Miller, Sean R.; Mills, Matthew N.; Palm, Russell F.; Regan, Samuel N.; Rishi, Anupam; Torres-Roca, Javier F.; Yu, Hsiang-Hsuan Michael; El Naqa, Issam",Intricacies of human-AI interaction in dynamic decision-making for precision oncology,NATURE COMMUNICATIONS,16,1,,,10.1038/s41467-024-55259-x,,Article,JAN 29 2025,2025,"AI decision support systems can assist clinicians in planning adaptive treatment strategies that can dynamically react to individuals' cancer progression for effective personalized care. However, AI's imperfections can lead to suboptimal therapeutics if clinicians over or under rely on AI. To investigate such collaborative decision-making process, we conducted a Human-AI interaction study on response-adaptive radiotherapy for non-small cell lung cancer and hepatocellular carcinoma. We investigated two levels of collaborative behavior: model-agnostic and model-specific; and found that Human-AI interaction is multifactorial and depends on the complex interrelationship between prior knowledge and preferences, patient's state, disease site, treatment modality, model transparency, and AI's learned behavior and biases. In summary, some clinicians may disregard AI recommendations due to skepticism; others will critically analyze AI recommendations on a case-by-case basis; clinicians will adjust their decisions if they find AI recommendations beneficial to patients; and clinician will disregard AI recommendations if deemed harmful or suboptimal and seek alternatives.",Human-AI Interaction and Perception,研究临床决策中医生与AI的互动，揭示信任、透明度与领域因素如何影响采纳AI建议。
"Koster, Raphael; Jan, Balaguer; Tacchetti, Andrea; Weinstein, Ari; Zhu, Tina; Hauser, Oliver; Williams, Duncan; Campbell-Gillingham, Lucy; Thacker, Phoebe; Botvinick, Matthew; Summerfield, Christopher",Human-centred mechanism design with Democratic AI,NATURE HUMAN BEHAVIOUR,6,10,1398,+,10.1038/s41562-022-01383-x,22-Jul,Article,22-Oct,2022,"Koster, Balaguer et al. show that an AI mechanism is able to learn to produce a redistribution policy which is preferred to alternatives by humans in an incentivized game.Building artificial intelligence (AI) that aligns with human values is an unsolved problem. Here we developed a human-in-the-loop research pipeline called Democratic AI, in which reinforcement learning is used to design a social mechanism that humans prefer by majority. A large group of humans played an online investment game that involved deciding whether to keep a monetary endowment or to share it with others for collective benefit. Shared revenue was returned to players under two different redistribution mechanisms, one designed by the AI and the other by humans. The AI discovered a mechanism that redressed initial wealth imbalance, sanctioned free riders and successfully won the majority vote. By optimizing for human preferences, Democratic AI offers a proof of concept for value-aligned policy innovation.",Impact on Human Cognition and Collective Intelligence,提出Democratic AI人机循环管道，利用强化学习设计出多数人偏好的再分配机制以对齐人类价值。
"Suzuki, Shoko",We need a culturally aware approach to AI,NATURE HUMAN BEHAVIOUR,,,,,10.1038/s41562-023-01738-y,23-Nov,Editorial Material; Early Access,,2023,"In Japan, people express gratitude towards technology and this helps them to achieve balance. Yet, dominant narratives teach us that anthropomorphizing artificial intelligence (AI) is not healthy. Our attitudes towards AI should not be bult upon overarching universal models, argues Shoko Suzuki.",Human-AI Interaction and Perception,主张在AI研究与实践中采用文化敏感的方法，反对以单一普世模型评判人机关系。
"Chanda, Tirtha; Haggenmueller, Sarah; Bucher, Tabea-Clara; Holland-Letz, Tim; Kittler, Harald; Tschandl, Philipp; Heppt, Markus V.; Berking, Carola; Utikal, Jochen S.; Schilling, Bastian; Buerger, Claudia; Navarrete-Dechent, Cristian; Goebeler, Matthias; Kather, Jakob Nikolas; Schneider, Carolin V.; Durani, Benjamin; Durani, Hendrike; Jansen, Martin; Wacker, Juliane; Wacker, Joerg; Brinker, Titus J.",Dermatologist-like explainable AI enhances melanoma diagnosis accuracy: eye-tracking study,NATURE COMMUNICATIONS,16,1,,,10.1038/s41467-025-59532-5,,Article,MAY 21 2025,2025,"Artificial intelligence (AI) systems substantially improve dermatologists' diagnostic accuracy for melanoma, with explainable AI (XAI) systems further enhancing their confidence and trust in AI-driven decisions. Despite these advancements, there remains a critical need for objective evaluation of how dermatologists engage with both AI and XAI tools. In this study, 76 dermatologists participate in a reader study, diagnosing 16 dermoscopic images of melanomas and nevi using an XAI system that provides detailed, domain-specific explanations, while eye-tracking technology assesses their interactions. Diagnostic performance is compared with that of a standard AI system lacking explanatory features. Here we show that XAI significantly improves dermatologists' diagnostic balanced accuracy by 2.8 percentage points compared to standard AI. Moreover, diagnostic disagreements with AI/XAI systems and complex lesions are associated with elevated cognitive load, as evidenced by increased ocular fixations. These insights have significant implications for the design of AI/XAI tools for visual tasks in dermatology and the broader development of XAI in medical diagnostics.",Applications in Professional and Scientific Fields,可解释AI在皮肤镜图像诊断中提高了皮肤科医生的准确率与信任，并通过眼动数据揭示认知负荷。
"Walter, Dror; Ophir, Yotam; Jamieson, Patrick E.; Jamieson, Kathleen Hall",Public perceptions of AI science and scientists relatively more negative but less politicized than general and climate science,PNAS NEXUS,4,6,,,10.1093/pnasnexus/pgaf163,,Article,25-Jun,2025,"Using a weighted 2023-2025 national probability panel of US adults, we compared the perceived Credibility, Prudence, Unbiasedness, Self-Correction, and Benefit (i.e. Factors Assessing Science's Self-Presentation [FASS]) of AI scientists with those of scientists in general and climate scientists in particular. Our analysis reveals that respondents' composite perceptions of AI scientists are the most negative of the three, a difference driven by a facet of the Prudence factor, specifically the perception that AI science is causing unintended consequences; political ideology and patterns of media exposure are substantially more predictive of perceptions of climate science and science in general than of AI; and FASS and respondent ideology predict more variance in support for federal funding of the other two than of AI.",Human-AI Interaction and Perception,美国公众对AI科学家的总体感知较为负面，主要源于对不良后果的谨慎担忧，政治化程度低于气候科学。
"Alvarez, Amanda; Caliskan, Aylin; Crockett, M. J.; Ho, Shirley S.; Messeri, Lisa; West, Jevin",Science communication with generative AI,NATURE HUMAN BEHAVIOUR,,,,,10.1038/s41562-024-01846-3,24-Mar,Article; Early Access,,2024,"Generative AI tools can quickly translate or summarize large volumes of complex information. This technology could revolutionize the way that we communicate science, but there are many reasons for caution. We asked six experts about the potential and pitfalls of generative AI for science communication.",Applications in Professional and Scientific Fields,讨论生成式AI在科学传播中快速处理与翻译信息的潜力及其须谨慎对待的陷阱。
"Capraro, Valerio; Lentsch, Austin; Acemoglu, Daron; Akgun, Selin; Akhmedova, Aisel; Bilancini, Ennio; Bonnefon, Jean-Francois; Branas-Garza, Pablo; Butera, Luigi; Douglas, Karen M.; Everett, Jim A. C.; Gigerenzer, Gerd; Greenhow, Christine; Hashimoto, Daniel A.; Holt-Lunstad, Julianne; Jetten, Jolanda; Johnson, Simon; Kunz, Werner H.; Longoni, Chiara; Lunn, Pete; Natale, Simone; Paluch, Stefanie; Rahwan, Iyad; Selwyn, Neil; Singh, Vivek; Suri, Siddharth; Sutcliffe, Jennifer; Tomlinson, Joe; van der Linden, Sander; Van Lange, Paul A. M.; Wall, Friederike; Van Bavel, Jay J.; Viale, Riccardo",The impact of generative artificial intelligence on socioeconomic inequalities and policy making,PNAS NEXUS,3,6,,,10.1093/pnasnexus/pgae191,24-Jun,Review,JUN 11 2024,2024,"Generative artificial intelligence (AI) has the potential to both exacerbate and ameliorate existing socioeconomic inequalities. In this article, we provide a state-of-the-art interdisciplinary overview of the potential impacts of generative AI on (mis)information and three information-intensive domains: work, education, and healthcare. Our goal is to highlight how generative AI could worsen existing inequalities while illuminating how AI may help mitigate pervasive social problems. In the information domain, generative AI can democratize content creation and access but may dramatically expand the production and proliferation of misinformation. In the workplace, it can boost productivity and create new jobs, but the benefits will likely be distributed unevenly. In education, it offers personalized learning, but may widen the digital divide. In healthcare, it might improve diagnostics and accessibility, but could deepen pre-existing inequalities. In each section, we cover a specific topic, evaluate existing research, identify critical gaps, and recommend research directions, including explicit trade-offs that complicate the derivation of a priori hypotheses. We conclude with a section highlighting the role of policymaking to maximize generative AI's potential to reduce inequalities while mitigating its harmful effects. We discuss strengths and weaknesses of existing policy frameworks in the European Union, the United States, and the United Kingdom, observing that each fails to fully confront the socioeconomic challenges we have identified. We propose several concrete policies that could promote shared prosperity through the advancement of generative AI. This article emphasizes the need for interdisciplinary collaborations to understand and address the complex challenges of generative AI.","Bias, Ethics, and Societal Risks",综述生成式AI在信息、工作、教育与医疗中可能加剧或缓解不平等，并提出政策与研究方向。
"Feuerriegel, Stefan; Shrestha, Yash Raj; von Krogh, Georg; Zhang, Ce",Bringing artificial intelligence to business management,NATURE MACHINE INTELLIGENCE,4,7,611,613,10.1038/s42256-022-00512-5,,Editorial Material,22-Jul,2022,"Artificial intelligence (AI) can support managers by effectively delegating management decisions to AI. There are, however, many organizational and technical hurdles that need to be overcome, and we offer a first step on this journey by unpacking the core factors that may hinder or foster effective decision delegation to AI.",Applications in Professional and Scientific Fields,探讨将AI引入管理决策委派的可能性及相关组织与技术障碍，提出初步框架。
"Scully, Jackie Leach",Disability and AI: Much more than assistive technologies.,"Science (New York, N.Y.)",389,6762,eaea4918,eaea4918,10.1126/science.aea4918,,Journal Article,2025-Aug-21,2025,"Nothing succeeds like success, we are told, and certainly that seems true for artificial intelligence (AI). It is hard to overstate either the ubiquity or the success of AI-based technologies in science and technology, health research and care, the media, and everyday life.","Bias, Ethics, and Societal Risks",阐述AI与残障议题的广泛关联，强调研究与政策应超越仅限辅助技术的视角。
"Piao, Jinghua; Liu, Jiazhen; Zhang, Fang; Su, Jun; Li, Yong",Human-AI adaptive dynamics drives the emergence of information cocoons,NATURE MACHINE INTELLIGENCE,5,11,1214,1224,10.1038/s42256-023-00731-4,23-Oct,Article,23-Nov,2023,"Despite AI-driven recommendation algorithms being widely adopted to counter information overload, substantial evidence suggests that they are building cocoons of homogeneous contents and viewpoints, further aggravating social polarization and prejudice. Curbing these perils requires a deep insight into the origin of information cocoons. Here we investigate information cocoons in the real world using two large datasets and find that a large number of users are trapped in information cocoons. Further empirical analysis suggests that two ingredients, each corresponding to a fundamental mechanism in human-AI interaction systems, are correlated with the loss of information diversity. Grounded on the empirical findings, we derive a mechanistic model for the adaptive information dynamics in complex human-AI interaction systems governed by these fundamental mechanisms. It allows us to predict critical transitions between three states: diversification, partial information cocoons, and deep information cocoons. Our work not only empirically traces real-world information cocoons in two representative scenarios, but also theoretically unearths basic mechanisms governing the emergence of information cocoons. We provide a theoretical method for understanding major social issues resulting from adaptive information dynamics in complex human-AI interaction systems.It is widely known that AI-based recommendation systems on social media and news websites can isolate humans from diverse information, eventually trapping them in so-called information cocoons, where they are exposed to a narrow range of viewpoints. Li et al. introduce an adaptive information dynamics model to uncover the origin of information cocoons in complex human-AI interaction systems, and test their findings on two large real-world datasets.",Impact on Human Cognition and Collective Intelligence,基于实证与模型揭示人机自适应互动如何导致信息茧房的形成及其关键转变机制。
"Ahsen, Mehmet Eren; Ayvaci, Mehmet U. S.; Mookerjee, Radha; Stolovitzky, Gustavo",Economics of AI and human task sharing for decision making in screening mammography,NATURE COMMUNICATIONS,16,1,,,10.1038/s41467-025-57409-1,,Article,MAR 7 2025,2025,"The rising global incidence of breast cancer and the persistent shortage of specialized radiologists have heightened the demand for innovative solutions in mammography screening. Artificial intelligence (AI) has emerged as a promising tool to bridge this demand-supply gap, with potential applications ranging from full automation to integrated AI-human decision-making. This study evaluates the economic feasibility of incorporating artificial intelligence (AI) into mammography screening within healthcare settings, considering full or partial integration. To evaluate the economic viability, we employ an optimization model specifically designed to minimize mammography screening costs. This model considers three distinct approaches when interpreting mammograms: automation strategy utilizing AI exclusively, delegation strategy involving the selective allocation of tasks between radiologists and AI, and the expert-alone strategy relying solely on radiologist decisions. Our findings underscore the significance of disease prevalence in relation to the trade-off between costs associated with false positives (e.g., follow-up expenses) and false negatives (e.g., litigation costs stemming from missed diagnoses) in shaping the AI strategy for healthcare organizations. We backtest our approach using data from an AI contest in which participants aimed to match or surpass radiologists' performance in assessing screening mammograms for women. The contest data supports the optimality of the delegation strategy, potentially leading to cost savings of 17.5% to 30.1% compared to relying solely on human experts. Our research provides guidance for healthcare organizations considering AI integration in mammography screening, with broader implications for work design and human-AI hybrid solutions in various fields.",Applications in Professional and Scientific Fields,通过优化模型评估AI在乳腺筛查中不同人机任务分配策略，发现委派策略能显著降低成本。
"McKee, Kevin R.; Botvinick, Matthew",AI learns to encourage group cooperation by making new connections,NATURE HUMAN BEHAVIOUR,7,10,1618,1619,10.1038/s41562-023-01699-2,23-Sep,Editorial Material,23-Oct,2023,"We trained an artificial intelligence (AI) system to recommend different interactions and connections between humans playing a group game together. Through trial and error, the AI system learned to take an encouraging approach to uncooperative individuals, keeping them engaged with the group and boosting cooperation levels for everyone.",Impact on Human Cognition and Collective Intelligence,训练AI通过建议新的社交连接来鼓励不合作者参与，从而整体提升群体合作水平。
"Wittenberg, Chloe; Epstein, Ziv; Peloquin-Skulski, Gabrielle; Berinsky, Adam J.; Rand, David G.",Labeling AI-generated media online,PNAS NEXUS,4,6,,,10.1093/pnasnexus/pgaf170,,Article,25-Jun,2025,"Recent advancements in generative AI have raised widespread concern about the use of this technology to spread audio and visual misinformation. In response, there has been a major push among policymakers and technology companies to label AI-generated media appearing online. It remains unclear, however, what types of labels are most effective for this purpose. Here, we evaluate two (potentially complementary) strategies for labeling AI-generated content online: (i) a process-based approach, aimed at clarifying how content was made and (ii) a harm-based approach, aimed at highlighting content's potential to mislead. Using two preregistered survey experiments focused on misleading, AI-generated images (total n = 7,579 Americans), we assess the consequences of these different labeling strategies for viewers' beliefs and behavioral intentions. Overall, we find that all of the labels we tested significantly decreased participants' belief in the presented claims. However, in both studies, labels that simply informed participants that content was generated using AI tended to have little impact on respondents' stated likelihood of engaging with their assigned post. Together, these results shed light on the relative advantages and disadvantages of different approaches to labeling AI-generated media online.",Content Authenticity and Mitigation Strategies,比较不同AI生成媒体标注策略对受众信念与行为意向的影响，发现标注可降低信任但对参与意愿影响有限。
"Messeri, Lisa; Crockett, M. J.",Artificial intelligence and illusions of understanding in scientific research,NATURE,627,8002,49,58,10.1038/s41586-024-07146-0,,Review,MAR 7 2024,2024,"Scientists are enthusiastically imagining ways in which artificial intelligence (AI) tools might improve research. Why are AI tools so attractive and what are the risks of implementing them across the research pipeline? Here we develop a taxonomy of scientists' visions for AI, observing that their appeal comes from promises to improve productivity and objectivity by overcoming human shortcomings. But proposed AI solutions can also exploit our cognitive limitations, making us vulnerable to illusions of understanding in which we believe we understand more about the world than we actually do. Such illusions obscure the scientific community's ability to see the formation of scientific monocultures, in which some types of methods, questions and viewpoints come to dominate alternative approaches, making science less innovative and more vulnerable to errors. The proliferation of AI tools in science risks introducing a phase of scientific enquiry in which we produce more but understand less. By analysing the appeal of these tools, we provide a framework for advancing discussions of responsible knowledge production in the age of AI.The proliferation of artificial intelligence tools in scientific research risks creating illusions of understanding, where scientists believe they understand more about the world than they actually do.",Impact on Human Cognition and Collective Intelligence,AI在科研中可能造成理解幻象，促成方法单一化并削弱创新。
"Purcell, Zoe A.; Bonnefon, Jean-Francois",Humans feel too special for machines to score their morals,PNAS NEXUS,2,6,,,10.1093/pnasnexus/pgad179,23-Jun,Article,MAY 31 2023,2023,"Artificial intelligence (AI) can be harnessed to create sophisticated social and moral scoring systems-enabling people and organizations to form judgments of others at scale. However, it also poses significant ethical challenges and is, subsequently, the subject of wide debate. As these technologies are developed and governing bodies face regulatory decisions, it is crucial that we understand the attraction or resistance that people have for AI moral scoring. Across four experiments, we show that the acceptability of moral scoring by AI is related to expectations about the quality of those scores, but that expectations about quality are compromised by people's tendency to see themselves as morally peculiar. We demonstrate that people overestimate the peculiarity of their moral profile, believe that AI will neglect this peculiarity, and resist for this reason the introduction of moral scoring by AI.",Human-AI Interaction and Perception,人们因自认为道德独特而对AI道德评分持抵触态度。
"Tey, Kian Siong; Mazar, Asaf; Tomaino, Geoff; Duckworth, Angela L.; Ungar, Lyle H.",People judge others more harshly after talking to bots,PNAS NEXUS,3,9,,,10.1093/pnasnexus/pgae397,,Article,SEP 24 2024,2024,"People now commonly interact with Artificial Intelligence (AI) agents. How do these interactions shape how humans perceive each other? In two preregistered studies (total N = 1,261), we show that people evaluate other humans more harshly after interacting with an AI (compared with an unrelated purported human). In Study 1, participants who worked on a creative task with AIs (versus purported humans) subsequently rated another purported human's work more negatively. Study 2 replicated this effect and demonstrated that the results hold even when participants believed their evaluation would not be shared with the purported human. Exploratory analyses of participants' conversations show that prior to their human evaluations they were more demanding, more instrumental and displayed less positive affect towards AIs (versus purported humans). These findings point to a potentially worrisome side effect of the exponential rise in human-AI interactions.",Human-AI Interaction and Perception,与AI交互后人们对他人评价更严厉，显示人机互动影响对人际评判。
"Agrawal, Ajay; Gans, Joshua S.; Goldfarb, Avi",Do we want less automation?,SCIENCE,381,6654,155,158,10.1126/science.adh9429,,Editorial Material,JUL 14 2023,2023,"Impressive achievements made through artificial intelligence (AI) innovations in automating the tasks required in many jobs have reinforced concerns about labor market disruption and increased income inequality. This has motivated calls for change in the direction of AI innovation from being guided by task automation to instead focusing on labor augmentation (1). But task automation and labor augmentation are not polar opposites. Instead, automation of some tasks can lead to augmentation of labor elsewhere. Furthermore, AI automation may provide a path to reversing the trend of increasing income inequality by enabling disproportionate productivity improvements for lower-wage workers, allowing them to perform at levels that would previously require years of education and experience.",Applications in Professional and Scientific Fields,任务自动化并非与劳动增强对立，自动化可提升低薪工人生产力并潜在缩减不平等。
"Pan, Xu; Schwartz, Odelia",Multimodal AI needs active human interaction,NATURE HUMAN BEHAVIOUR,8,10,1825,1826,10.1038/s41562-024-01932-6,24-Jul,Letter,24-Oct,2024,"As AI tools quickly become more capable, multimodal and pervasive in daily life, it is important to actively collaborate with them in ways that promote - rather than inhibit - human skill development.",Impact on Human Cognition and Collective Intelligence,多模态AI应通过主动人机协作以促进而非抑制人类技能发展。
"Sourati, Jamshid; Evans, James A.",Accelerating science with human-aware artificial intelligence,NATURE HUMAN BEHAVIOUR,7,10,,,10.1038/s41562-023-01648-z,23-Jul,Article,23-Oct,2023,"Artificial intelligence (AI) models trained on published scientific findings have been used to invent valuable materials and targeted therapies, but they typically ignore the human scientists who continually alter the landscape of discovery. Here we show that incorporating the distribution of human expertise by training unsupervised models on simulated inferences that are cognitively accessible to experts dramatically improves (by up to 400%) AI prediction of future discoveries beyond models focused on research content alone, especially when relevant literature is sparse. These models succeed by predicting human predictions and the scientists who will make them. By tuning human-aware AI to avoid the crowd, we can generate scientifically promising `alien' hypotheses unlikely to be imagined or pursued without intervention until the distant future, which hold promise to punctuate scientific advance beyond questions currently pursued. By accelerating human discovery or probing its blind spots, human-aware AI enables us to move towards and beyond the contemporary scientific frontier.",Applications in Professional and Scientific Fields,通过将人类专业分布纳入训练，人机感知的AI显著提升科学预测并能提出非主流但有前途的假设。
"Zhou, Eric B.; Lee, Dokyun; Gu, Bin",Who expands the human creative frontier with generative AI: Hive minds or masterminds?,SCIENCE ADVANCES,11,36,,,10.1126/sciadv.adu5800,,Article,SEP 3 2025,2025,"Artists are rapidly integrating generative text-to-image models into their workflows, yet how this affects creative discovery remains unclear. Leveraging large-scale data from an online art platform, we compare artificial intelligence (AI)-assisted creators to matched nonadopters to assess novel idea contributions. Initially, a concentrated subset of AI-assisted creators contributes more novel artifacts in absolute terms through increased output-the productivity effect-although the average rate of contributing novel artifacts decreases because of a dilution effect. This reflects a shift toward high-volume, incremental exploration, ultimately yielding a greater aggregate of novel artifacts by AI-assisted creators. We observe no evidence of a human-AI effect above and beyond the productivity effect. The release of open-source Stable Diffusion accelerates novel contributions across a more diverse group, suggesting that text-to-image tools facilitate exploration at scale, initially enabling persistent breakthroughs by select masterminds, driven by increased volume, and subsequently enabling widespread novel contributions from a hive mind.",Impact on Human Cognition and Collective Intelligence,生成式AI初期由少数高产创作者推动突破，随后通过降低门槛促成更广泛的集体创新。
"Pataranutaporn, Pat; Danry, Valdemar; Leong, Joanne; Punpongsanon, Parinya; Novy, Dan; Maes, Pattie; Sra, Misha",AI-generated characters for supporting personalized learning and well-being,NATURE MACHINE INTELLIGENCE,3,12,1013,1022,10.1038/s42256-021-00417-9,21-Dec,Article,21-Dec,2021,"Advancements in machine learning have recently enabled the hyper-realistic synthesis of prose, images, audio and video data, in what is referred to as artificial intelligence (AI)-generated media. These techniques offer novel opportunities for creating interactions with digital portrayals of individuals that can inspire and intrigue us. AI-generated portrayals of characters can feature synthesized faces, bodies and voices of anyone, from a fictional character to a historical figure, or even a deceased family member. Although negative use cases of this technology have dominated the conversation so far, in this Perspective we highlight emerging positive use cases of AI-generated characters, specifically in supporting learning and well-being. We demonstrate an easy-to-use AI character generation pipeline to enable such outcomes and discuss ethical implications as well as the need for including traceability to help maintain trust in the generated media. As we look towards the future, we foresee generative media as a crucial part of the ever growing landscape of human-AI interaction.Digitally recreating the likeness of a person used to be a costly and complex process. Through the use of generative models, AI-generated characters can now be made with relative ease. Pataranutaporn et al. discuss in this Perspective how this technology can be used for positive applications in education and well-being.",Applications in Professional and Scientific Fields,生成式角色可用于个性化学习与福祉支持，但需可追溯性与伦理监管以维持信任。
"Jiang, Liwei; Hwang, Jena D.; Bhagavatula, Chandra; Bras, Ronan Le; Liang, Jenny T.; Levine, Sydney; Dodge, Jesse; Sakaguchi, Keisuke; Forbes, Maxwell; Hessel, Jack; Borchardt, Jon; Sorensen, Taylor; Gabriel, Saadia; Tsvetkov, Yulia; Etzioni, Oren; Sap, Maarten; Rini, Regina; Choi, Yejin",Investigating machine moral judgement through the Delphi experiment,NATURE MACHINE INTELLIGENCE,7,1,,,10.1038/s42256-024-00969-6,25-Jan,Article,25-Jan,2025,"As our society adopts increasingly powerful artificial intelligence (AI) systems for pervasive use, there are growing concerns about machine morality-or lack thereof. Millions of users already rely on the outputs of AI systems, such as chatbots, as decision aids. Meanwhile, AI researchers continue to grapple with the challenge of aligning these systems with human morality and values. In response to this challenge, we build and test Delphi, an open-source AI system trained to predict the moral judgements of US participants. The computational framework of Delphi is grounded in the framework proposed by the prominent moral philosopher John Rawls. Our results speak to the promises and limits of teaching machines about human morality. Delphi demonstrates improved generalization capabilities over those exhibited by off-the-shelf neural language models. At the same time, Delphi's failures also underscore important challenges in this arena. For instance, Delphi has limited cultural awareness and is susceptible to pervasive biases. Despite these shortcomings, we demonstrate several compelling use cases of Delphi, including its incorporation as a component within an ensemble of AI systems. Finally, we computationally demonstrate the potential of Rawls's prospect of hybrid approaches for reliable moral reasoning, inspiring future research in computational morality.","Bias, Ethics, and Societal Risks",Delphi系统能预测美国受试者的道德判断但受文化局限与偏见影响，凸显机器道德判断的局限。
"Chanda, Tirtha; Hauser, Katja; Hobelsberger, Sarah; Bucher, Tabea-Clara; Garcia, Carina Nogueira; Wies, Christoph; Kittler, Harald; Tschandl, Philipp; Navarrete-Dechent, Cristian; Podlipnik, Sebastian; Chousakos, Emmanouil; Crnaric, Iva; Majstorovic, Jovana; Alhajwan, Linda; Foreman, Tanya; Peternel, Sandra; Sarap, Sergei; Ozdemir, Irem; Barnhill, Raymond L.; Llamas-Velasco, Mar; Poch, Gabriela; Korsing, Soeren; Sondermann, Wiebke; Gellrich, Frank Friedrich; Heppt, Markus V.; Erdmann, Michael; Haferkamp, Sebastian; Drexler, Konstantin; Goebeler, Matthias; Schilling, Bastian; Utikal, Jochen S.; Ghoreschi, Kamran; Froehling, Stefan; Krieghoff-Henning, Eva; Brinker, Titus J.",Dermatologist-like explainable AI enhances trust and confidence in diagnosing melanoma,NATURE COMMUNICATIONS,15,1,,,10.1038/s41467-023-43095-4,,Article,JAN 15 2024,2024,"Artificial intelligence (AI) systems have been shown to help dermatologists diagnose melanoma more accurately, however they lack transparency, hindering user acceptance. Explainable AI (XAI) methods can help to increase transparency, yet often lack precise, domain-specific explanations. Moreover, the impact of XAI methods on dermatologists' decisions has not yet been evaluated. Building upon previous research, we introduce an XAI system that provides precise and domain-specific explanations alongside its differential diagnoses of melanomas and nevi. Through a three-phase study, we assess its impact on dermatologists' diagnostic accuracy, diagnostic confidence, and trust in the XAI-support. Our results show strong alignment between XAI and dermatologist explanations. We also show that dermatologists' confidence in their diagnoses, and their trust in the support system significantly increase with XAI compared to conventional AI. This study highlights dermatologists' willingness to adopt such XAI systems, promoting future use in the clinic.Artificial intelligence has become popular as a cancer classification tool, but there is distrust of such systems due to their lack of transparency. Here, the authors develop an explainable AI system which produces text- and region-based explanations alongside its classifications which was assessed using clinicians' diagnostic accuracy, diagnostic confidence, and their trust in the system.",Applications in Professional and Scientific Fields,提供领域化可解释说明的AI可提升皮肤科医生的诊断准确性、信心与对系统的信任。
"Wang, Ying-Lang; Huang, Mao-Chih",Human-AI team halves cost of designing step,NATURE,616,7958,667,668,10.1038/d41586-023-01353-x,,Editorial Material,APR 27 2023,2023,"Engineers and algorithms have competed in a virtual test to design a step in the process of manufacturing computer chips. Pairing human expertise with computational efficiency proves most cost-effective, but only when the timing is right.",Applications in Professional and Scientific Fields,在人机协作中，恰当时机结合人工专长与算法可显著降低设计成本。
"Wells, Sarah","Ready or not, AI is coming to science education - and students have opinions",NATURE,628,8007,459,461,10.1038/d41586-024-01002-x,,Editorial Material,APR 11 2024,2024,"As educators debate whether it's even possible to use AI safely in research and education, students are taking a role in shaping its responsible use.",Applications in Professional and Scientific Fields,学生在AI进入科学教育的讨论中积极表达意见，影响其负责任使用的制定。
"Spitale, Giovanni; Biller-Andorno, Nikola; Germani, Federico",AI model GPT-3 (dis)informs us better than humans,SCIENCE ADVANCES,9,26,,,10.1126/sciadv.adh1850,,Article,23-Jun,2023,"Artificial intelligence (AI) is changing the way we create and evaluate information, and this is happening during an infodemic, which has been having marked effects on global health. Here, we evaluate whether recruited individuals can distinguish disinformation from accurate information, structured in the form of tweets, and determine whether a tweet is organic or synthetic, i.e., whether it has been written by a Twitter user or by the AI model GPT-3. The results of our preregistered study, including 697 participants, show that GPT-3 is a double-edge sword: In comparison with humans, it can produce accurate information that is easier to understand, but it can also produce more compelling disinformation. We also show that humans cannot distinguish between tweets generated by GPT-3 and written by real Twitter users. Starting from our results, we reflect on the dangers of AI for disinformation and on how information campaigns can be improved to benefit global health.",Content Authenticity and Mitigation Strategies,GPT-3能同时生成更易懂的准确信息和更具说服力的虚假信息，且人类难以分辨来源真伪。
[Anonymous],Data sovereignty in genomics and medical research,NATURE MACHINE INTELLIGENCE,4,11,905,906,10.1038/s42256-022-00578-1,22-Nov,Editorial Material,22-Nov,2022,"AI promises to bring many benefits to healthcare and research, but mistrust has built up owing to many instances of harm to under-represented communities. To amend this, participatory approaches can directly involve communities in AI research that will impact them. An important element of such approaches is ensuring that communities can take control over their own data and how they are shared.","Bias, Ethics, and Societal Risks",基因组与医疗研究需尊重数据主权，采用参与式方法让社区掌控其数据共享。
"Tzachor, Asaf; Devare, Medha; King, Brian; Avin, Shahar; Heigeartaigh, Sean O.",Responsible artificial intelligence in agriculture requires systemic understanding of risks and externalities,NATURE MACHINE INTELLIGENCE,4,2,104,109,10.1038/s42256-022-00440-4,,Article,22-Feb,2022,"Global agriculture is poised to benefit from the rapid advance and diffusion of artificial intelligence (AI) technologies. AI in agriculture could improve crop management and agricultural productivity through plant phenotyping, rapid diagnosis of plant disease, efficient application of agrochemicals and assistance for growers with location-relevant agronomic advice. However, the ramifications of machine learning (ML) models, expert systems and autonomous machines for farms, farmers and food security are poorly understood and under-appreciated. Here, we consider systemic risk factors of AI in agriculture. Namely, we review risks relating to interoperability, reliability and relevance of agricultural data, unintended socio-ecological consequences resulting from ML models optimized for yields, and safety and security concerns associated with deployment of ML platforms at scale. As a response, we suggest risk-mitigation measures, including inviting rural anthropologists and applied ecologists into the technology design process, applying frameworks for responsible and human-centred innovation, setting data cooperatives for improved data transparency and ownership rights, and initial deployment of agricultural AI in digital sandboxes.Machine learning applications in agriculture can bring many benefits in crop management and productivity. However, to avoid harmful effects of a new round of technological modernization, fuelled by AI, a thorough risk assessment is required, to review and mitigate risks such as unintended socio-ecological consequences and security concerns associated with applying machine learning models at scale.","Bias, Ethics, and Societal Risks",农业AI带来增产潜力但亦有互操作性、生态与安全风险，需系统性评估与人本设计。
"Starke, Christopher; Ventura, Alfio; Bersch, Clara; Cha, Meeyoung; de Vreese, Claes; Doebler, Philipp; Dong, Mengchen; Kraemer, Nicole; Leib, Margarita; Peter, Jochen; Schaefer, Lea; Soraperra, Ivan; Szczuka, Jessica; Tuchtfeld, Erik; Wald, Rebecca; Koebis, Nils",Risks and protective measures for synthetic relationships,NATURE HUMAN BEHAVIOUR,8,10,1834,1836,10.1038/s41562-024-02005-4,,Article,24-Oct,2024,"As artificial intelligence tools become more sophisticated, humans build synthetic relationships with them. Synthetic relationships differ fundamentally from traditional human-machine interactions and present new risks, such as privacy breaches, psychological manipulation and the erosion of human autonomy. This necessitates proactive, human-centred policies.","Bias, Ethics, and Societal Risks",人类与AI的合成关系带来隐私、操控与自主性侵蚀等新风险，需以人为本制定防护政策。
"Tessler, Michael Henry; Bakker, Michiel A.; Jarrett, Daniel; Sheahan, Hannah; Chadwick, Martin J.; Koster, Raphael; Evans, Georgina; Campbell-Gillingham, Lucy; Collins, Tantum; Parkes, David C.; Botvinick, Matthew; Summerfield, Christopher",AI can help humans find common ground in democratic deliberation,SCIENCE,386,6719,,,10.1126/science.adq2852,,Article,OCT 18 2024,2024,"Finding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants' personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens' assembly involving a demographically representative sample of the UK population.",Impact on Human Cognition and Collective Intelligence,作为调解者的AI能生成更具信息性与中立性的共识陈述，促使公众讨论收敛。
"Hutson, Matthew",GUINEA PIGBOTS,SCIENCE,381,6654,121,123,,,Editorial Material,JUL 14 2023,2023,Doing research with human subjects is costly and cumbersome. Can AI chatbots replace them?,Applications in Professional and Scientific Fields,探讨用AI聊天机器人替代人类被试以降低实验成本与复杂性。
"Dong, Mengchen; Bonnefon, Jean-Francois; Rahwan, Iyad",Heterogeneous preferences and asymmetric insights for AI use among welfare claimants and non-claimants,NATURE COMMUNICATIONS,16,1,,,10.1038/s41467-025-62440-3,,Article,JUL 29 2025,2025,"The deployment of AI in welfare benefit allocation accelerates decision-making but has led to unfair denials and false fraud accusations. In the US and UK (N = 3249), we examine public acceptability of speed-accuracy trade-offs among claimants and non-claimants. While the public generally tolerates modest accuracy losses for faster decisions, claimants are less willing to accept AI in welfare systems, raising concerns that using aggregate data for calibration could misalign policies with the preferences of those most affected. Our study further uncovers asymmetric insights between claimants and non-claimants. Non-claimants overestimate claimants' willingness to accept speed-accuracy trade-offs, even when financially incentivized for accurate perspective-taking. This suggests that policy decisions aimed at supporting vulnerable groups may need to incorporate minority voices beyond popular opinion, as non-claimants may not easily understand claimants' perspectives.","Bias, Ethics, and Societal Risks",福利制度中使用AI决策时，受影响群体比旁观者更反对，存在偏离受众意愿的政策风险。
"Mahner, Florian P.; Muttenthaler, Lukas; Guclu, Umut; Hebart, Martin N.",Dimensions underlying the representational alignment of deep neural networks with humans,NATURE MACHINE INTELLIGENCE,7,6,848,859,10.1038/s42256-025-01041-7,,Article,25-Jun,2025,"Determining the similarities and differences between humans and artificial intelligence (AI) is an important goal in both computational cognitive neuroscience and machine learning, promising a deeper understanding of human cognition and safer, more reliable AI systems. Much previous work comparing representations in humans and AI has relied on global, scalar measures to quantify their alignment. However, without explicit hypotheses, these measures only inform us about the degree of alignment, not the factors that determine it. To address this challenge, we propose a generic framework to compare human and AI representations, based on identifying latent representational dimensions underlying the same behaviour in both domains. Applying this framework to humans and a deep neural network (DNN) model of natural images revealed a low-dimensional DNN embedding of both visual and semantic dimensions. In contrast to humans, DNNs exhibited a clear dominance of visual over semantic properties, indicating divergent strategies for representing images. Although in silico experiments showed seemingly consistent interpretability of DNN dimensions, a direct comparison between human and DNN representations revealed substantial differences in how they process images. By making representations directly comparable, our results reveal important challenges for representational alignment and offer a means for improving their comparability.",Impact on Human Cognition and Collective Intelligence,提出框架比较人类与深度网络的潜在表征维度，发现DNN偏向视觉而非语义，揭示对齐挑战。
"Kidd, Celeste; Birhane, Abeba",How AI can distort human beliefs,SCIENCE,380,6651,1222,1223,10.1126/science.adi0248,,Editorial Material,23-Jun,2023,"Models can convey biases and false information to usersIndividual humans form their beliefs by sampling a small subset of the available data in the world. Once those beliefs are formed with high certainty, they can become stubborn to revise. Fabrication and bias in generative artificial intelligence (AI) models are established phenomena that can occur as part of regular system use, in the absence of any malevolent forces seeking to push bias or disinformation. However, transmission of false information and bias from these models to people has been prominently absent from the discourse. Overhyped, unrealistic, and exaggerated capabilities permeate how generative AI models are presented, which contributes to the popular misconception that these models exceed human-level reasoning and exacerbates the risk of transmission of false information and negative stereotypes to people.",Impact on Human Cognition and Collective Intelligence,生成式AI的偏见与虚构内容可被用户采样并固化为顽固信念，从而扭曲人类信念。
"Sengupta, Nandana; Subramanian, Vidya; Mukhopadhyay, Anwesh; Scaria, Arul George",A Global South perspective for ethical algorithms and the State,NATURE MACHINE INTELLIGENCE,5,3,184,186,10.1038/s42256-023-00621-9,23-Feb,Editorial Material,23-Mar,2023,"We explore the intersection between algorithms and the State from the perspectives of legislative action, public perception and the use of AI in public administration. Taking India as a case study, we discuss the potential fallout from the absence of rigorous scholarship on such questions for countries in the Global South.","Bias, Ethics, and Societal Risks",以印度为例讨论算法与国家在立法、公共感知与行政中交互的伦理与风险，强调全球南方研究缺位。
"Dominijanni, Giulia; Shokur, Solaiman; Salvietti, Gionata; Buehler, Sarah; Palmerini, Erica; Rossi, Simone; De Vignemont, Frederique; d'Avella, Andrea; Makin, Tamar R.; Prattichizzo, Domenico; Micera, Silvestro",The neural resource allocation problem when enhancing human bodies with extra robotic limbs,NATURE MACHINE INTELLIGENCE,3,10,850,860,10.1038/s42256-021-00398-9,,Review,21-Oct,2021,"The emergence of robotic body augmentation provides exciting innovations that will revolutionize the fields of robotics, human-machine interaction and wearable electronics. Although augmentative devices such as extra robotic arms and fingers are informed by restorative technologies in many ways, they also introduce unique challenges for bidirectional human-machine collaboration. Can humans adapt and learn to operate a new robotic limb collaboratively with their biological limbs, without restricting other physical abilities? To successfully achieve robotic body augmentation, we need to ensure that, by giving a user an additional (artificial) limb, we are not trading off the functionalities of an existing (biological) one. Here, we introduce the 'neural resource allocation problem' and discuss how to allow the effective voluntary control of augmentative devices without compromising control of the biological body. In reviewing the relevant literature on extra robotic fingers and arms, we critically assess the range of potential solutions available for this neural resource allocation problem. For this purpose, we combine multiple perspectives from engineering and neuroscience with considerations including human-machine interaction, sensory-motor integration, ethics and law. In summary, we aim to define common foundations and operating principles for the successful implementation of robotic body augmentation.The development of extra fingers and arms is an exciting research area in robotics, human-machine interaction and wearable electronics. It is unclear, however, whether humans can adapt and learn to control extra limbs and integrate them into a new sensorimotor representation, without sacrificing their natural abilities. The authors review this topic and describe challenges in allocating neural resources for robotic body augmentation.",Impact on Human Cognition and Collective Intelligence,提出“神经资源分配问题”，评估人类如何学习控制额外机器人肢体而不牺牲生物肢体功能。
"Kanarik, Keren J.; Osowiecki, Wojciech T.; Lu, Yu (Joe); Talukder, Dipongkar; Roschewsky, Niklas; Park, Sae Na; Kamon, Mattan; Fried, David M.; Gottscho, Richard A.",Human-machine collaboration for improving semiconductor process development,NATURE,616,7958,707,+,10.1038/s41586-023-05773-7,23-Mar,Article,APR 27 2023,2023,"One of the bottlenecks to building semiconductor chips is the increasing cost required to develop chemical plasma processes that form the transistors and memory storage cells(1,2). These processes are still developed manually using highly trained engineers searching for a combination of tool parameters that produces an acceptable result on the silicon wafer(3). The challenge for computer algorithms is the availability of limited experimental data owing to the high cost of acquisition, making it difficult to form a predictive model with accuracy to the atomic scale. Here we study Bayesian optimization algorithms to investigate how artificial intelligence (AI) might decrease the cost of developing complex semiconductor chip processes. In particular, we create a controlled virtual process game to systematically benchmark the performance of humans and computers for the design of a semiconductor fabrication process. We find that human engineers excel in the early stages of development, whereas the algorithms are far more cost-efficient near the tight tolerances of the target. Furthermore, we show that a strategy using both human designers with high expertise and algorithms in a human first-computer last strategy can reduce the cost-to-target by half compared with only human designers. Finally, we highlight cultural challenges in partnering humans with computers that need to be addressed when introducing artificial intelligence in developing semiconductor processes.",Applications in Professional and Scientific Fields,在人机协作中结合人类专家與贝叶斯优化算法可在半导体工艺开发中显著降低成本并互补优势。
"Guilfoos, Todd",Social media: generative AI could harm mental health,NATURE,617,7962,676,676,10.1038/d41586-023-01694-7,,Letter,MAY 25 2023,2023,Letter to the Editor,"Bias, Ethics, and Societal Risks",致信警示生成式AI在社交媒体上可能对心理健康造成危害。
"Yoshida, Kazunari",Mentor-trainee dialogue on proper use of AI tools,NATURE,624,7992,523,523,10.1038/d41586-023-04062-7,,Letter,DEC 21 2023,2023,Letter to the Editor,Human-AI Interaction and Perception,导师与受训者就AI工具的正确使用展开对话，讨论实践规范（信函形式）。
"Luo, Yiyue; Liu, Chao; Lee, Young Joong; Delpreto, Joseph; Wu, Kui; Foshey, Michael; Rus, Daniela; Palacios, Tomas; Li, Yunzhu; Torralba, Antonio; Matusik, Wojciech",Adaptive tactile interaction transfer via digitally embroidered smart gloves,NATURE COMMUNICATIONS,15,1,,,10.1038/s41467-024-45059-8,,Article,JAN 29 2024,2024,"Human-machine interfaces for capturing, conveying, and sharing tactile information across time and space hold immense potential for healthcare, augmented and virtual reality, human-robot collaboration, and skill development. To realize this potential, such interfaces should be wearable, unobtrusive, and scalable regarding both resolution and body coverage. Taking a step towards this vision, we present a textile-based wearable human-machine interface with integrated tactile sensors and vibrotactile haptic actuators that are digitally designed and rapidly fabricated. We leverage a digital embroidery machine to seamlessly embed piezoresistive force sensors and arrays of vibrotactile actuators into textiles in a customizable, scalable, and modular manner. We use this process to create gloves that can record, reproduce, and transfer tactile interactions. User studies investigate how people perceive the sensations reproduced by our gloves with integrated vibrotactile haptic actuators. To improve the effectiveness of tactile interaction transfer, we develop a machine-learning pipeline that adaptively models how each individual user reacts to haptic sensations and then optimizes haptic feedback parameters. Our interface showcases adaptive tactile interaction transfer through the implementation of three end-to-end systems: alleviating tactile occlusion, guiding people to perform physical skills, and enabling responsive robot teleoperation.Adaptive tactile interactions transfer across users, space, and time, via embroidered smart gloves is reported by the authors. The scalable fabrication and adaptive computation pipeline enable tactile occlusion alleviation, human skills transfer, and interactive teleoperation.",Human-AI Interaction and Perception,通过数字刺绣将传感器与振动执行器集成手套，并用机器学习个性化调控以实现触觉记录与远程再现。
"Bago, Bence; Bonnefon, Jean-Francois",Generative AI as a tool for truth,SCIENCE,385,6714,1164,1165,10.1126/science.ads0433,,Editorial Material,SEP 13 2024,2024,Conversation with a trained chatbot can reduce conspiratorial beliefs,Impact on Human Cognition and Collective Intelligence,与训练型聊天机器人对话能够减少阴谋论信念，表明生成式AI可用作纠错认知工具。
"Grote, Thomas; Freiesleben, Timo; Berens, Philipp",Foundation models in healthcare require rethinking reliability,NATURE MACHINE INTELLIGENCE,6,12,1421,1423,10.1038/s42256-024-00924-5,24-Nov,Article,24-Dec,2024,"A new class of AI models, called foundation models, has entered healthcare. Foundation models violate several basic principles of the standard machine learning paradigm for assessing reliability, making it necessary to rethink what guarantees are required to establish warranted trust in them.",Applications in Professional and Scientific Fields,指出基础模型进入医疗领域后挑战既有可靠性评估范式，需重新思考建立信任的保证措施。
"Yan, Lixiang; Greiff, Samuel; Teuber, Ziwen; Gasevic, Dragan",Promises and challenges of generative artificial intelligence for human learning,NATURE HUMAN BEHAVIOUR,8,10,1839,1850,10.1038/s41562-024-02004-5,,Review,24-Oct,2024,"Generative artificial intelligence (GenAI) holds the potential to transform the delivery, cultivation and evaluation of human learning. Here the authors examine the integration of GenAI as a tool for human learning, addressing its promises and challenges from a holistic viewpoint that integrates insights from learning sciences, educational technology and human-computer interaction. GenAI promises to enhance learning experiences by scaling personalized support, diversifying learning materials, enabling timely feedback and innovating assessment methods. However, it also presents critical issues such as model imperfections, ethical dilemmas and the disruption of traditional assessments. Thus, cultivating AI literacy and adaptive skills is imperative for facilitating informed engagement with GenAI technologies. Rigorous research across learning contexts is essential to evaluate GenAI's effect on human cognition, metacognition and creativity. Humanity must learn with and about GenAI, ensuring that it becomes a powerful ally in the pursuit of knowledge and innovation, rather than a crutch that undermines our intellectual abilities.This Perspective describes the roles of generative AI in providing personalized support, diversity and innovative assessment in learning. However, it also raises ethical concerns and highlights issues such as model imperfection, underscoring the need for AI literacy and adaptability.",Impact on Human Cognition and Collective Intelligence,综述生成式AI在个性化学习、反馈与评估中的潜力，同时强调模型缺陷、伦理问题与对认知影响的研究必要性。
"Crawford, Kate",Time to regulate AI that interprets human emotions,NATURE,592,7853,167,167,10.1038/d41586-021-00868-5,,Editorial Material,APR 8 2021,2021,The pandemic is being used as a pretext to push unproven artificial-intelligence tools into workplaces and schools.,"Bias, Ethics, and Societal Risks",呼吁监管解读人类情绪的AI工具，批评疫情被借用推动未验证技术进入工作与学校场景。
"Sanderson, Katharine",GPT-4 is here: what scientists think,NATURE,615,7954,773,773,10.1038/d41586-023-00816-5,,News Item,MAR 30 2023,2023,Researchers are excited about the AI - but many are frustrated that its underlying engineering is cloaked in secrecy.,Human-AI Interaction and Perception,科学界对GPT‑4既感到兴奋，又因其工程细节保密而感到沮丧。
